<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>General Intuition - Research Lab</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">

    <style>
        /* CSS */
        body {
            margin: 0;
            overflow: hidden; /* Prevent scrollbars */
            font-family: 'Inter', sans-serif;
            color: #eee;
            background-color: #020210; /* Ensure background is set */
            touch-action: none; /* Prevent default touch actions like scrolling/zooming on the page */
            user-select: none; /* Standard */
            -webkit-user-select: none; /* Safari/Chrome */
            -moz-user-select: none; /* Firefox */
            -ms-user-select: none; /* IE/Edge */
            -webkit-user-drag: none; /* Disable dragging */
            overscroll-behavior-x: none; /* Prevent swipe navigation */
        }
        canvas {
            display: block;
             touch-action: manipulation; /* Allows pinch zoom etc. handled by JS */
             user-select: none;
             -webkit-user-select: none;
             -moz-user-select: none;
             -ms-user-select: none;
             -webkit-user-drag: none;
             overscroll-behavior-x: none; /* Also apply here for good measure */
         }
        #info-box, #instructions {
            position: absolute;
            background-color: rgba(0, 0, 0, 0.8);
            border-radius: 8px;
            padding: 10px 15px;
            font-size: 13px;
            z-index: 10;
            line-height: 1.6;
        }
        #info-box {
            top: 20px;
            left: 20px;
            border: 1px solid #0f0; /* Default highlight */
            color: #eee; /* Default text color */
            max-width: 320px;
            display: none;
            pointer-events: none;
            text-shadow: none; /* Remove default text shadow */
        }
        #info-box h3 {
            margin-top: 0;
            margin-bottom: 8px;
            font-size: 15px;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: #eee; /* Default header color */
            text-shadow: none;
        }
        #info-box.hover-concept h3 { color: #0f0; text-shadow: 0 0 3px #0f0; }
        #info-box.hover-paper h3 { color: #0ff; text-shadow: 0 0 3px #0ff; }
        #info-box.hover-team h3 { color: #ffd700; text-shadow: 0 0 3px #ffd700; }
        #info-box.hover-concept { border-color: #0f0; }
        #info-box.hover-paper { border-color: #0ff; }
        #info-box.hover-team { border-color: #ffd700; }


        #instructions {
            bottom: 15px;
            left: 50%;
            transform: translateX(-50%);
            color: #aaa;
            font-size: 11px;
            padding: 6px 12px;
            text-align: center;
        }
        /* --- Mobile Reticle (Enlarged) --- */
        #mobile-reticle {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 96px;
            height: 96px;
            border: 2px solid rgba(255, 255, 255, 0.3); /* Make border less opaque */
            border-radius: 50%;
            pointer-events: none; /* Doesn't interfere with other interactions */
            display: none; /* Hidden by default, shown via JS for mobile */
            z-index: 50; /* Above canvas, below overlays */
            box-sizing: border-box;
            transition: border-color 0.2s ease; /* Smooth transition on target lock */
        }
        /* Style for the inner dot of the reticle (Enlarged) */
         #mobile-reticle::after {
             content: '';
             position: absolute;
             top: 50%;
             left: 50%;
             transform: translate(-50%, -50%);
             width: 16px;
             height: 16px;
             background-color: rgba(255, 255, 255, 0.3); /* Make dot less opaque */
             border-radius: 50%;
             transition: background-color 0.2s ease; /* Smooth transition on target lock */
         }
         /* Style when reticle is over a target */
         #mobile-reticle.target-locked {
             border-color: rgba(0, 255, 0, 0.7); /* Brighter green border */
         }
         #mobile-reticle.target-locked::after {
              background-color: rgba(0, 255, 0, 0.7); /* Brighter green dot */
         }


        #instructions a {
            color: #0ff;
            text-decoration: none;
            display: block;
            margin-top: 6px;
            cursor: pointer;
            font-size: 12px;
        }
        #instructions a:hover { text-decoration: underline; }

        /* --- Overlay Styles --- */
        #animation-overlay, #disclaimer-overlay {
            position: fixed; top: 0; left: 0; width: 100%; height: 100%;
            display: none; justify-content: center; align-items: center;
            z-index: 100; padding: 20px; box-sizing: border-box;
            font-family: 'Inter', sans-serif;
             touch-action: auto; /* Allow scrolling within disclaimer if needed */
        }
        #animation-overlay { background-color: rgba(0, 0, 0, 0.9); flex-direction: column; }
        #disclaimer-overlay { background-color: rgba(10, 10, 20, 0.95); overflow-y: auto; }

        #overlay-node-label { color: #bbb; font-size: 16px; margin-bottom: 20px; text-align: center; font-weight: 700; }
        #overlay-content { display: flex; align-items: center; justify-content: space-around; width: 90%; max-width: 700px; height: 80px; margin-bottom: 25px; position: relative; border-bottom: 1px dashed rgba(0, 255, 255, 0.3); padding-bottom: 20px; }
        .network-layer { width: 8px; height: 100%; background: linear-gradient(to bottom, rgba(0, 100, 100, 0.3), rgba(0, 200, 200, 0.6), rgba(0, 100, 100, 0.3)); border-radius: 4px; box-shadow: 0 0 6px rgba(0, 255, 255, 0.4); opacity: 0; animation: fadeInLayer 0.3s forwards; transition: box-shadow 0.1s linear, background 0.1s linear; position: relative; overflow: hidden; }
        .network-layer::before { content: ''; position: absolute; top: 0; left: 0; width: 100%; height: 100%; background: repeating-linear-gradient( 45deg, rgba(255,255,255,0.0), rgba(255,255,255,0.0) 2px, rgba(255,255,255,0.05) 2px, rgba(255,255,255,0.05) 4px ); opacity: 0; transition: opacity 0.1s linear; }
        .network-layer:nth-child(1) { animation-delay: 0.02s; } .network-layer:nth-child(2) { animation-delay: 0.04s; } .network-layer:nth-child(3) { animation-delay: 0.06s; } .network-layer:nth-child(4) { animation-delay: 0.08s; } .network-layer:nth-child(5) { animation-delay: 0.1s; }
        .network-layer.active { box-shadow: 0 0 15px #0f0; background: linear-gradient(to bottom, rgba(0, 255, 0, 0.3), rgba(100, 255, 100, 1), rgba(0, 255, 0, 0.3)); }
        .network-layer.active::before { opacity: 0.5; }
        #signal-wave { position: absolute; left: -30px; top: 50%; height: 20px; transform: translateY(-50%); opacity: 0; }
        .signal-dot-item { position: absolute; width: 5px; height: 5px; background-color: #0f0; border-radius: 50%; box-shadow: 0 0 5px #0f0; }
        #word-selection-vis { position: absolute; bottom: -15px; left: 50%; transform: translateX(-50%); width: auto; min-width: 120px; padding: 4px; background-color: rgba(0, 0, 0, 0.7); border: 1px solid #444; border-radius: 4px; text-align: center; opacity: 0; transition: opacity 0.1s ease-out; pointer-events: none; display: flex; justify-content: center; align-items: center; gap: 6px; }
        #word-selection-vis.visible { opacity: 1; }
        .prob-word { font-size: 11px; padding: 2px 5px; border-radius: 3px; }
        .prob-word.selected { color: #0f0; border: 1px solid #0f0; background-color: rgba(0, 255, 0, 0.2); font-weight: 700; }
        .prob-word.alternative { color: #888; border: 1px solid #444; background-color: rgba(80, 80, 80, 0.2); opacity: 0.7; }
        #overlay-output-container { width: 90%; max-width: 700px; min-height: 60px; text-align: left; border: 1px dashed #0ff; padding: 12px; border-radius: 5px; background-color: rgba(0, 50, 50, 0.3); margin-top: 15px; }
        #overlay-output { font-size: 15px; color: #0f0; text-shadow: 0 0 5px #0f0; line-height: 1.7; }
        #overlay-output .current-word { background-color: rgba(0, 255, 0, 0.3); padding: 0 2px; border-radius: 2px; animation: fadeHighlight 0.5s forwards; }
        #overlay-explanation { color: #0ff; font-size: 12px; margin-top: 20px; max-width: 600px; text-align: center; line-height: 1.6; text-decoration: underline; }
        #overlay-explanation a { color: #0ff; text-decoration: none;}
        #overlay-explanation a:hover { color: #3ff; }

        .close-overlay-btn {
            position: absolute; top: 15px; right: 15px; background-color: #dd0000; color: white; border: none; padding: 6px 12px; border-radius: 5px; cursor: pointer;
            font-family: 'Inter', sans-serif; font-size: 13px; font-weight: 700;
            box-shadow: 0 0 8px #ff0000; transition: background-color 0.2s;
        }
        .close-overlay-btn:hover { background-color: #aa0000; }

        /* --- Disclaimer Overlay Styles --- */
        #disclaimer-content {
            background-color: rgba(0, 0, 0, 0.6); border: 1px solid #0ff; border-radius: 8px;
            padding: 25px 35px; max-width: 650px; /* Adjusted max-width */
            width: 90%; color: #ccc; font-size: 14px; line-height: 1.8;
            font-family: 'Inter', sans-serif;
            text-align: center; /* Center content */
         }
        #disclaimer-content h2 {
             font-family: 'Press Start 2P', cursive;
            color: #0ff; font-size: 20px; margin-bottom: 25px; text-align: center; text-shadow: 0 0 5px #0ff; letter-spacing: 1px;
        }
        #disclaimer-content h3 {
            font-family: 'Press Start 2P', cursive;
            color: #eee; font-size: 15px; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px dashed #555; padding-bottom: 8px; letter-spacing: 0.5px;
            display: inline-block; /* Keep border only under text */
            width: auto; /* Adjust width */
        }
        #disclaimer-content p, #disclaimer-content li { margin-bottom: 12px; }
        #disclaimer-content strong { color: #fff; font-weight: 700; }
        #disclaimer-content ul { list-style: none; padding-left: 0; margin-left: 0; }
        #team-list {
            margin-bottom: 30px; /* Add space below team list */
            display: inline-block; /* Center the list block */
            text-align: left; /* Align list items left */
        }
        #team-list li { margin-bottom: 8px; }
        #team-list li a {
            color: #ffd700;
            text-decoration: none;
            font-weight: 700;
            font-size: 14px;
        }
        #team-list li a:hover { text-decoration: underline; color: #fff; }
        /* Style for the email link */
        #disclaimer-content .email-link {
            color: #0ff;
            text-decoration: none;
            font-size: 14px;
            margin-top: 15px;
            display: block; /* Make it block to center easily */
        }
         #disclaimer-content .email-link:hover {
            text-decoration: underline;
            color: #3ff;
        }


        /* --- Animations --- */
        @keyframes fadeInLayer { from { opacity: 0; transform: scaleY(0.5); } to { opacity: 1; transform: scaleY(1); } }
        @keyframes nodeClickFlash { 0%, 100% { transform: scale(1.0); } 50% { transform: scale(1.5); } }
        @keyframes fadeHighlight { from { background-color: rgba(0, 255, 0, 0.3); } to { background-color: transparent; } }
    </style>
</head>
<body class="font-sans">
    <div id="main-visualization">
        <div id="info-box"><h3 id="info-label">...</h3><p id="info-description">...</p></div>
        <div id="mobile-reticle"></div>
        <div id="instructions">
             <span id="desktop-instructions">Click & Drag: Rotate | Scroll: Zoom | Hover: Inspect</span>
             <span id="mobile-instructions" style="display: none;">Drag: Rotate | Pinch: Zoom | Center Reticle: Inspect</span>
            <a id="show-disclaimer-link" href="#">About / Info</a>
        </div>
        <canvas id="representation-canvas"></canvas>
    </div>

    <div id="animation-overlay">
         <div id="overlay-node-label">Exploring Concept: ...</div>
        <div id="overlay-content">
            <div class="network-layer"></div> <div class="network-layer"></div> <div class="network-layer"></div> <div class="network-layer"></div> <div class="network-layer"></div>
            <div id="signal-wave"></div>
             <div id="word-selection-vis"></div>
        </div>
        <div id="overlay-output-container">
            <p id="overlay-output"></p>
        </div>
         <div id="overlay-explanation">
             careers@generalintuition.ai
         </div>
        <button id="close-animation-overlay-btn" class="close-overlay-btn">X</button>
    </div>

    <div id="disclaimer-overlay">
         <div id="disclaimer-content">
            <h2>General Intuition</h2>

            <ul id="team-list">
                </ul>

            <a href="mailto:careers@generalintuition.ai" class="email-link">careers@generalintuition.ai</a>

        </div>
        <button id="close-disclaimer-overlay-btn" class="close-overlay-btn">X</button>
    </div>

    <script type="importmap">
        { "imports": { "three": "https://cdn.jsdelivr.net/npm/three@0.163.0/build/three.module.js", "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.163.0/examples/jsm/" } }
    </script>

    <script type="module">
        import * as THREE from 'three';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        // --- Configuration ---
        const conceptNodeSize = 0.7; // Concepts slightly larger
        const paperNodeSize = 0.6; // Papers slightly smaller than concepts
        const teamNodeSize = 0.6; // Team same size as papers
        const paperNodeColor = 0x00ffff; // Cyan
        const teamNodeColor = 0xffd700; // Gold
        const highlightColor = 0x00ff00; // Green (Concept hover)
        const paperHighlightColor = 0x00ffff; // Cyan (Paper hover - same as base)
        const teamHighlightColor = 0xffd700; // Gold (Team hover - same as base)
        const clickHighlightColor = 0xffffff; // White (Concept click)
        const defaultSpaceColor = 0xcccccc; // Light gray for inactive concept/paper
        const numNetworkLayers = 5;
        const clickEffectDuration = 500; // ms for click visual pulse
        const transitionDelay = 300; // ms delay before overlay shows on concept click
        const conceptNodeSpread = 4; // How far concepts spread around category centroids
        const paperNodeSpreadRadius = 30; // Radius for distributing papers
        const teamNodeSpread = 5; // How far team nodes spread around origin

        // Node Movement Params
        const nodeMoveAmplitude = 0.3; // Wiggle distance
        const nodeMoveFreq = 0.4; // Wiggle speed
        const nodeLerpFactor = 0.02; // Wiggle smoothness (lower is smoother)

        // Team Probe Movement Params
        const teamMoveSpeed = 0.01; // Speed towards target paper
        const teamTargetReachedThreshold = 2.5; // Node stops when distance is less than this
        const teamPauseDuration = 2500; // Pause time in milliseconds at paper

        // Proximity Color Change Params
        const proximityThreshold = 5.0; // How close a team node needs to be to activate another node
        const colorLerpFactor = 0.08; // Color transition smoothness

        // Cluster Line Params (Re-added)
        const lineDefaultColor = 0x444444; // Dark gray inactive lines
        const lineDefaultOpacity = 0.08; // Slightly lower default opacity
        const lineActiveColor = 0xffffff; // White active lines
        const lineMaxActiveOpacity = 0.30; // Slightly lower max active opacity
        const lineOpacityIncreasePerProbe = 0.05; // How much opacity increases per paused team node
        const lineTransitionSpeed = 0.1; // Line color/opacity smoothness

        // Background Params
        const backgroundColor = 0x020210; // Very dark blue/black
        const fogColor = backgroundColor;
        const fogNear = 80;
        const fogFar = 200;
        const starCount = 7000;
        const starSphereRadius = 400;

        // Animation Params (Node Scaling & Base Wiggle)
        const baseAnimFreq = 0.8; // Base frequency for subtle size oscillation
        const baseAnimAmpConcept = 0.03; // Base amplitude for concept node size oscillation
        const baseAnimAmpPaperTeam = 0.05; // Base amplitude for paper/team node size oscillation
        const distanceScaleFactor = 15.0; // Distance at which nodes start scaling down
        const maxDistanceScaleBonus = 0.4; // Max size increase for close nodes

        // Faster Animation Timings (Overlay)
        const layerPassDuration = 240; // Total time for signal wave to cross layers
        const wordSelectionDuration = 90; // Time the word probabilities are shown
        const wordAppendDelay = 20; // Delay between appending words
        const numSignalDots = 5; // Dots in the signal wave animation

        // Mobile Click Prevention
        const mobileClickTimeThreshold = 200; // Clicks within this ms of touchstart are ignored

        // Concept Color Palette (for proximity activation)
        const conceptColorPalette = [
            0x4a90e2, 0xbd10e0, 0xaaaaaa, 0x7ed321, 0xf5a623, 0x50e3c2, 0xe04457,
        ];
        let conceptColorIndex = 0;

        // --- Helper Functions ---
        function randomPosition(baseX, baseY, baseZ, spread) {
            return new THREE.Vector3(
                baseX + (Math.random() * spread - spread / 2),
                baseY + (Math.random() * spread - spread / 2),
                baseZ + (Math.random() * spread - spread / 2)
            );
        }
        const genericAlternatives = ["concept", "process", "state"]; // Alternatives for LLM overlay animation
        const delay = ms => new Promise(resolve => setTimeout(resolve, ms)); // Simple async delay
        function vectorsAreEqual(v1, v2, tolerance = 0.01) {
            // Check if both vectors exist before calculating distance
            return v1 && v2 && v1.distanceTo(v2) < tolerance;
        }


        // --- Node Data ---
        // Original categories data (used only to get concept labels/info)
        const categories = {
             "Perception": { labels: ["Sensor Fusion", "Proprioception", "Visual Odometry", "SLAM", "Object Recognition", "Depth Sensing", "Tactile Sensing", "Auditory Scene"], infoPrefix: "Sensing:", sentencePrefix: "Making sense of the world involves techniques like Sensor Fusion, Proprioception, and Visual Odometry." },
             "Action/Control": { labels: ["Reinforcement Learning", "Inverse Kinematics", "PID Control", "Motion Planning", "Grasping", "Locomotion", "Trajectory Optimization", "Imitation Learning"], infoPrefix: "Acting:", sentencePrefix: "Generating physical behavior relies on methods such as Reinforcement Learning, Inverse Kinematics, and Motion Planning." },
             "Cognition/Planning": { labels: ["Task Planning", "World Model", "Predictive Coding", "Affordance Theory", "Symbolic Reasoning", "Goal Setting", "Decision Making", "Memory Systems"], infoPrefix: "Thinking:", sentencePrefix: "Formulating plans for interaction involves concepts like Task Planning, World Models, and Symbolic Reasoning." },
             "Embodiment": { labels: ["Morphology", "Physical Interaction", "Degrees of Freedom", "Actuators", "Simulation Gap", "Embodied Cognition", "Soft Robotics", "Bio-inspiration"], infoPrefix: "Form:", sentencePrefix: "The physical form and its interaction capabilities are defined by aspects such as Morphology, Degrees of Freedom, and Actuators." },
             "Core AI/ML": { labels: ["Neural Network", "Transformer", "Attention Mechanism", "Backpropagation", "Gradient Descent", "Latent Space", "CNN", "RNN", "GAN"], infoPrefix: "Algorithms:", sentencePrefix: "Extracting patterns from data often utilizes core AI/ML techniques like Neural Networks, Transformers, and Backpropagation." },
             "Learning Paradigms": { labels: ["Supervised", "Unsupervised", "Self-Supervised", "Continual Learning", "Meta-Learning", "Transfer Learning", "Active Learning", "Online Learning"], infoPrefix: "Adaptation:", sentencePrefix: "Improving capabilities often involves different learning paradigms such as Supervised, Unsupervised, and Self-Supervised learning." },
             "Human-Robot Interaction": { labels: ["Social Robotics", "Trust in Automation", "Shared Autonomy", "Teleoperation", "Gesture Recognition", "Natural Language Interface", "Explainable AI (XAI)"], infoPrefix: "Collaboration:", sentencePrefix: "Facilitating human-robot collaboration relies on concepts like Social Robotics, Trust in Automation, and Shared Autonomy." },
             "Robotic Manipulation": { labels: ["Dexterous Manipulation", "Force Control", "Assembly Tasks", "In-Hand Manipulation", "Regrasping", "Compliant Control", "Peg-in-Hole"], infoPrefix: "Handling:", sentencePrefix: "Physically engaging with objects requires capabilities such as Dexterous Manipulation, Force Control, and Assembly Task execution." },
             "Navigation & Mapping": { labels: ["Path Planning", "Global Localization", "Topological Maps", "Grid Maps", "Exploration Algorithms", "Multi-Agent Path Finding", "Semantic Mapping"], infoPrefix: "Movement:", sentencePrefix: "Moving effectively through environments depends on techniques like Path Planning, Global Localization, and various Mapping strategies." },
             "Computer Vision": { labels: ["Image Segmentation", "Pose Estimation", "Scene Understanding", "3D Reconstruction", "Optical Flow", "Feature Detection", "Video Analysis"], infoPrefix: "Seeing:", sentencePrefix: "Interpreting visual input often involves Computer Vision techniques such as Image Segmentation, Pose Estimation, and Scene Understanding." },
             "Natural Language Processing": { labels: ["Language Models (LLMs)", "Sentiment Analysis", "Machine Translation", "Question Answering", "Text Summarization", "Named Entity Recognition", "Embeddings"], infoPrefix: "Language:", sentencePrefix: "Grounding communication often involves Natural Language Processing methods like Language Models (LLMs), Sentiment Analysis, and Machine Translation." },
             "Knowledge Representation": { labels: ["Ontologies", "Knowledge Graphs", "Logical Inference", "Semantic Web", "Rule-Based Systems", "Description Logics", "Common Sense Reasoning"], infoPrefix: "Knowledge:", sentencePrefix: "Structuring world knowledge utilizes methods such as Ontologies, Knowledge Graphs, and Logical Inference." },
             "Optimization Methods": { labels: ["Gradient-Based Opt.", "Evolutionary Algorithms", "Bayesian Optimization", "Convex Optimization", "Integer Programming", "Search Algorithms (A*, BFS)"], infoPrefix: "Optimizing:", sentencePrefix: "Finding optimal solutions or sequences often involves Optimization Methods like Gradient-Based Optimization, Evolutionary Algorithms, and Search Algorithms." },
             "Simulation & Modeling": { labels: ["Physics Engines", "Digital Twins", "System Identification", "Agent-Based Modeling", "Finite Element Analysis", "Sim-to-Real Transfer"], infoPrefix: "Modeling:", sentencePrefix: "Anticipating system behavior and consequences often uses Simulation & Modeling tools like Physics Engines, Digital Twins, and System Identification." },
             "Cognitive Architectures": { labels: ["SOAR", "ACT-R", "Connectionism", "Symbolic Systems", "Hybrid Architectures", "Developmental Robotics", "Modular Architectures"], infoPrefix: "Mind Models:", sentencePrefix: "Integrating diverse cognitive capabilities often relies on Cognitive Architectures such as SOAR, ACT-R, and Hybrid Systems." },
             "Neuroscience Inspiration": { labels: ["Spiking Neural Nets", "Hebbian Learning", "Hippocampal Models", "Cortical Columns", "Neural Plasticity", "Motor Cortex Models"], infoPrefix: "Brain-Inspired:", sentencePrefix: "Drawing inspiration from biology involves studying mechanisms like Spiking Neural Networks, Hebbian Learning, and Neural Plasticity." },
             "Philosophy of Mind": { labels: ["Consciousness", "Qualia", "Intentionality", "Free Will", "Mind-Body Problem", "Embodied Mind Thesis", "Extended Mind"], infoPrefix: "Mind Concepts:", sentencePrefix: "Exploring fundamental questions about mind and agency involves concepts like Consciousness, Intentionality, and the Mind-Body Problem." },
             "Ethics & Safety": { labels: ["AI Alignment", "Value Learning", "Bias Detection", "Fairness Metrics", "Robustness", "Interpretability", "Accountability"], infoPrefix: "Safety:", sentencePrefix: "Ensuring safe, reliable, and ethical AI behavior involves addressing areas like AI Alignment, Value Learning, Bias Detection, and Robustness." },
             "Multi-Agent Systems": { labels: ["Coordination", "Cooperation", "Competition", "Swarm Intelligence", "Distributed Control", "Negotiation"], infoPrefix: "Groups:", sentencePrefix: "Coordinating multiple interacting agents requires techniques for Coordination, Cooperation, Competition, and Swarm Intelligence." },
             "Probabilistic Methods": { labels: ["Bayesian Networks", "Kalman Filters", "Particle Filters", "Gaussian Processes", "Markov Models (HMMs)", "Probabilistic Programming"], infoPrefix: "Uncertainty:", sentencePrefix: "Handling uncertainty and noisy data often involves Probabilistic Methods like Bayesian Networks, Kalman Filters, and Markov Models." },
             "Geometric Methods": { labels: ["Lie Groups (SE3)", "Manifold Learning", "Computational Geometry", "Topology", "Differential Geometry", "Configuration Space"], infoPrefix: "Geometry:", sentencePrefix: "Reasoning about spatial relationships, configurations, and transformations often uses Geometric Methods including Lie Groups, Manifold Learning, and Computational Geometry." },
             "Information Theory": { labels: ["Entropy", "Mutual Information", "KL Divergence", "Compression", "Channel Capacity", "Coding Theory"], infoPrefix: "Information:", sentencePrefix: "Quantifying information, uncertainty, and communication efficiency uses concepts from Information Theory such as Entropy, Mutual Information, and KL Divergence." },
             "Control Theory": { labels: ["Linear Control", "Nonlinear Control", "Adaptive Control", "Optimal Control", "Robust Control", "Stochastic Control"], infoPrefix: "Dynamics:", sentencePrefix: "Achieving stable and desired system behavior often requires principles from Control Theory, including Linear Control, Adaptive Control, and Optimal Control." },
             "Hardware & Sensors": { labels: ["LIDAR", "IMU", "Cameras", "Force Sensors", "Encoders", "Microcontrollers", "FPGAs", "GPUs"], infoPrefix: "Hardware:", sentencePrefix: "The physical interface for robotic systems includes Hardware & Sensors like LIDAR, IMUs, Cameras, and processing units such as GPUs." },
             "Data Structures & Algo": { labels: ["Graphs", "Trees", "Hash Tables", "Sorting", "Dynamic Programming", "Complexity Theory"], infoPrefix: "Computation:", sentencePrefix: "Efficient computation and data management relies on fundamental Data Structures & Algorithms like Graphs, Trees, Sorting, and Dynamic Programming." },
             "Quantum Computing": { labels: ["Qubits", "Superposition", "Entanglement", "Quantum Gates", "Quantum Algorithms", "Quantum ML"], infoPrefix: "Quantum:", sentencePrefix: "Exploring future computational paradigms involves understanding Quantum Computing concepts such as Qubits, Superposition, Entanglement, and Quantum Algorithms." },
             "Biologically Plausible Learning": { labels: ["Synaptic Plasticity", "Neuromodulation", "Homeostasis", "Local Learning Rules", "Spike-Timing-Dependent Plasticity (STDP)"], infoPrefix: "Bio Learning:", sentencePrefix: "Developing learning mechanisms inspired by biology involves exploring concepts like Synaptic Plasticity, Neuromodulation, and Spike-Timing-Dependent Plasticity (STDP)." },
             "Developmental AI": { labels: ["Intrinsic Motivation", "Curiosity-Driven Learning", "Sensorimotor Stages", "Affordance Learning", "Skill Acquisition"], infoPrefix: "Growth:", sentencePrefix: "Acquiring skills progressively, akin to natural development, involves concepts like Intrinsic Motivation, Curiosity-Driven Learning, and Sensorimotor Stages." },
             "Causality": { labels: ["Causal Inference", "Structural Causal Models", "Do-Calculus", "Counterfactuals", "Intervention"], infoPrefix: "Cause/Effect:", sentencePrefix: "Understanding cause-and-effect relationships and action outcomes requires reasoning about Causality using tools like Causal Inference, Structural Causal Models, and Counterfactuals." },
             "Theory of Computation": { labels: ["Turing Machines", "Computability", "Complexity Classes (P vs NP)", "Automata Theory", "Formal Languages"], infoPrefix: "Computation Theory:", sentencePrefix: "Understanding the fundamental limits and capabilities of computation relates to the Theory of Computation, including concepts like Turing Machines, Computability, and Complexity Classes." },
              "Cybernetics": { labels: ["Feedback Loops", "Control Systems", "Communication Theory", "Self-Regulation", "Homeostasis (biological)"], infoPrefix: "Systems:", sentencePrefix: "Understanding systems that maintain stability and achieve goals through feedback involves principles of Cybernetics, such as Feedback Loops, Control Systems, and Self-Regulation." },
              "Spacecraft Operations": { labels: ["Asteroid Navigation", "Orbital Docking", "Deep Space Probe", "Fleet Coordination", "Emergency Maneuvers", "Resource Scanning", "Interstellar Travel", "Lander Control"], infoPrefix: "Space Ops:", sentencePrefix: "Autonomous control in space involves complex tasks like Asteroid Navigation, Orbital Docking, Deep Space Probe management, and Lander Control." },
              "Human Intuition": { labels: ["Gut Feeling", "Implicit Learning", "Heuristics", "Expert Judgment", "Analogical Reasoning", "Emotional Intelligence"], infoPrefix: "Human Intuition:", sentencePrefix: "Understanding human cognition involves exploring phenomena like Gut Feelings, Implicit Learning, Heuristics, and Expert Judgment." },
              "Machine Intuition": { labels: ["Emergent Behavior", "Subsymbolic Reasoning", "Latent Representations", "Policy Gradients (RL)", "Generative Models", "Few-Shot Adaptation"], infoPrefix: "Machine Intuition:", sentencePrefix: "Developing AI capable of intuitive-like responses involves fostering Emergent Behavior, Subsymbolic Reasoning, and leveraging Latent Representations from models like Policy Gradients and Generative Models." },
             // Team category is defined but not used for concepts now
             // "Team": { labels: [], infoPrefix: "Team:", sentencePrefix: "Defining the collaborative structure and goals." }
         };

        // --- Input Data ---
        const teamMembersInput = [
            { name: "Eloi Alonso", url: "https://eloialonso.github.io/" },
            { name: "Adam Jelley", url: "https://adamjelley.github.io/" },
            { name: "Vincent Micheli", url: "https://vmicheli.github.io/" },
            { name: "Pim de Witte", url: "https://pimdewitte.com/" },
            { name: "Fredrik Noren", url: "https://www.linkedin.com/in/fredrik-noren/" },
            { name: "Florian Laurent", url: "https://masterscrat.github.io/" },
            { name: "Karthick Jeyapal", url: "https://www.linkedin.com/in/karthick-jeyapal-a9a4a524/" }
        ];
        const existingPapersInput = [ /* ... paper data ... */
            { name: "JARVIS-VLA", url: "https://x.com/_akhaliq/status/1903155708457013610?s=12&t=TGTk_lVC4Ugrh9Op2GBFbQ", category: "Human-Robot Interaction", fullName: "JARVIS-VLA: A Generalist Vision-Language-Action Model for Robotic Control" },
            { name: "Gemma3", url: "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf", category: "Natural Language Processing", fullName: "Gemma3 Technical Report" },
            { name: "Portal Agents", url: "https://zhongwen.one/projects/portal/", category: "Multi-Agent Systems", fullName: "Agents Play Thousands of 3D Video Games through Procedurally Generated Worlds (Portal)" },
            { name: "LLDMs", url: "https://arxiv.org/abs/2502.09992", category: "Core AI/ML", fullName: "Large Language Diffusion Models" },
            { name: "GRPO From Scratch", url: "https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb", category: "Optimization Methods", fullName: "Coding GRPO from Scratch: A Guide to Distributed Implementation" },
            { name: "Magma", url: "https://microsoft.github.io/Magma/", category: "Core AI/ML", fullName: "Magma: A Foundation Model for Multimodal AI Agents" },
            { name: "Helix", url: "https://www.figure.ai/news/helix", category: "Embodiment", fullName: "Helix: A Vision-Language-Action Model for Generalist Humanoid Control" },
            { name: "Scale on TPUs", url: "https://jax-ml.github.io/scaling-book/", category: "Hardware & Sensors", fullName: "How to Scale Your Model: A Systems View of LLMs on TPUs" },
            { name: "Ultra-Scale Playbook", url: "https://huggingface.co/spaces/nanotron/ultrascale-playbook", category: "Hardware & Sensors", fullName: "The Ultra-Scale Playbook: Training LLMs on GPU Clusters" },
            { name: "Gameplay Ideation", url: "https://www.nature.com/articles/s41586-025-08600-3", category: "Cognition/Planning", fullName: "World and Human Action Models towards gameplay ideation" },
            { name: "Value RL Scaling", url: "https://arxiv.org/abs/2502.04327", category: "Action/Control", fullName: "Value-Based Deep RL Scales Predictably" },
            { name: "Agent Scaling Laws", url: "https://arxiv.org/pdf/2411.04434", category: "Learning Paradigms", fullName: "Scaling Laws for Pre-training Agents and World Models" },
            { name: "NSA Attention", url: "https://x.com/deepseek_ai/status/1891745487071609327", category: "Core AI/ML", fullName: "NSA: A Hardware-Aligned and Natively Trainable Sparse Attention mechanism" },
            { name: "VideoJAM", url: "https://hila-chefer.github.io/videojam-paper.github.io/", category: "Computer Vision", fullName: "VideoJAM: Generating Videos with Motion Models and Subject Control" },
            { name: "OpenVLA", url: "https://openvla.github.io/", category: "Human-Robot Interaction", fullName: "OpenVLA: An Open-Source Vision-Language-Action Model" },
            { name: "π0 Policy", url: "https://www.physicalintelligence.company/blog/pi0", category: "Action/Control", fullName: "π0: Our First Generalist Policy" },
            { name: "Spotify: Jim Fan", url: "https://open.spotify.com/episode/2YEslWY161A5nAniNse3gR?si=vNtk6EVETxqNWPHCayo9ow&nd=1&dlsi=06144c246de64a1c", category: "Embodiment", fullName: "Spotify Jim Fan on Nvidia’s Embodied AI Lab and Jensen Huang’s Prediction" },
            { name: "Human-level FPS RL", url: "https://www.davidsilver.uk/wp-content/uploads/2020/03/ctf_compressed.pdf", category: "Multi-Agent Systems", fullName: "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning" },
            { name: "PaliGemma 2", url: "https://arxiv.org/pdf/2412.03555", category: "Core AI/ML", fullName: "PaliGemma 2: A Family of Versatile VLMs for Transfer" },
            { name: "Kimi k1.5", url: "https://github.com/MoonshotAI/Kimi-k1.5", category: "Action/Control", fullName: "Kimi k1.5: Scaling Reinforcement Learning with LLMs" },
            { name: "Compressed Video Action", url: "https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Compressed_Video_Action_CVPR_2018_paper.pdf", category: "Computer Vision", fullName: "Compressed Video Action Recognition" },
            { name: "SFT vs RL Generalization", url: "https://arxiv.org/pdf/2501.17161v1", category: "Learning Paradigms", fullName: "SFT Memorizes, RL Generalizes: A Comparative Study" },
            { name: "Cosmos World Model", url: "https://arxiv.org/pdf/2501.03575v1", category: "Cognition/Planning", fullName: "Cosmos World Foundation Model Platform for Physical AI" },
            { name: "GR00T N1", url: "https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T_1_Whitepaper.pdf", category: "Embodiment", fullName: "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots" },
            { name: "Genie Presentation", url: "https://www.youtube.com/watch?v=vs5a2JTy0K0", category: "Simulation & Modeling", fullName: "Genie UCL After dark presentation" },
            { name: "FAST Action Tokenization", url: "https://www.pi.website/research/fast", category: "Action/Control", fullName: "FAST: Efficient Action Tokenization for Vision-Language-Action Models" },
            { name: "Gemini Robotics", url: "https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/", category: "Embodiment", fullName: "Gemini Robotics: Bringing AI into the Physical World" },
            { name: "SIMA Agent", url: "https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/", category: "Multi-Agent Systems", fullName: "A generalist AI agent for 3D virtual environments (SIMA)" },
            { name: "Gaming, Goats & GI", url: "https://www.youtube.com/watch?v=64pndvbbokA", category: "Cognition/Planning", fullName: "Gaming, Goats & General Intelligence with Frederic Besse" },
            { name: "Mastering Control (World Models)", url: "https://arxiv.org/abs/2301.04104", category: "Cognition/Planning", fullName: "Mastering diverse control tasks through world models" },
            { name: "Pandora World Model", url: "https://world-model.maitrix.org/", category: "Cognition/Planning", fullName: "Pandora: Towards General World Model with Natural Language Actions and Video States" },
            { name: "Navigation World Models", url: "https://arxiv.org/pdf/2412.03572", category: "Navigation & Mapping", fullName: "Navigation World Models" },
            { name: "Seaweed-7B", url: "https://arxiv.org/abs/2504.08685", category: "Computer Vision", fullName: "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model" },
            { name: "Unified World Models", url: "https://weirdlabuw.github.io/uwm/", category: "Simulation & Modeling", fullName: "Unified World Models: Coupling Video and Action Diffusion" },
            { name: "UI-TARS", url: "https://seed-tars.com/1.5/", category: "Human-Robot Interaction", fullName: "UI-TARS: Pioneering Automated GUI Interaction with Native Agents" },
        ];
        const newPapersInput = [ /* ... paper data ... */
             { name: "Agent57", url: "https://arxiv.org/abs/2003.13350?utm_source=chatgpt.com", category: "Action/Control", fullName: "Agent57: Outperforming the Atari Human Benchmark" },
             { name: "Other-Play", url: "https://arxiv.org/pdf/2003.02979", category: "Multi-Agent Systems", fullName: "“Other-Play” for Zero-Shot Coordination" },
             { name: "NetHack Env", url: "https://arxiv.org/abs/2006.13760", category: "Simulation & Modeling", fullName: "The NetHack Learning Environment" },
             { name: "ReBeL", url: "https://arxiv.org/abs/2007.13544?utm_source=chatgpt.com", category: "Multi-Agent Systems", fullName: "Combining Deep Reinforcement Learning and Search for Imperfect-Information Games (ReBeL)" },
             { name: "XLand", url: "https://arxiv.org/abs/2107.12808?utm_source=chatgpt.com", category: "Developmental AI", fullName: "Open-Ended Learning Leads to Generally Capable Agents (XLand)" },
             { name: "VPT (New Link)", url: "https://arxiv.org/abs/2206.11795?utm_source=chatgpt.com", category: "Computer Vision", fullName: "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos" },
             { name: "MineDojo", url: "https://arxiv.org/pdf/2206.08853", category: "Simulation & Modeling", fullName: "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge" },
             { name: "DeepNash", url: "https://arxiv.org/abs/2206.15378?utm_source=chatgpt.com", category: "Multi-Agent Systems", fullName: "Mastering the Game of Stratego with Model-Free Multi-Agent RL (DeepNash)" },
             { name: "CICERO", url: "https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf?utm_source=chatgpt.com", category: "Multi-Agent Systems", fullName: "Human-Level Play in Diplomacy (CICERO)" },
             { name: "Voyager (New Link)", url: "https://arxiv.org/abs/2305.16291", category: "Embodiment", fullName: "Voyager: An Open-Ended Embodied Agent with Large Language Models" },
             { name: "Diffusion WM", url: "https://diamond-wm.github.io/", category: "Cognition/Planning", fullName: "Diffusion for World Modeling" },
        ];
        const allPapersInput = [...existingPapersInput, ...newPapersInput];

        // --- Processed Node Data ---
        const paperNodesData = [];
        const teamNodesData = [];
        const conceptNodesData = []; // Filtered concepts
        const paperPositions = []; // To store paper positions for team targeting and line drawing
        const paperNodeMap = new Map(); // Map category to list of papers in that category
        const categoryCentroids = new Map(); // Map category name to centroid position

        // 1. Process Papers: Determine positions and group by category
        const numPapers = allPapersInput.length;
        const radius = paperNodeSpreadRadius;
        allPapersInput.forEach((paper, i) => {
            // Calculate position using spherical distribution (Fibonacci lattice/Golden Angle)
            const phi = Math.acos(-1 + (2 * i) / numPapers); // Angle from +Y axis
            const theta = Math.sqrt(numPapers * Math.PI) * phi; // Angle around Y axis

            const x = radius * Math.sin(phi) * Math.cos(theta);
            const y = radius * Math.cos(phi);
            const z = radius * Math.sin(phi) * Math.sin(theta);
            const paperPos = new THREE.Vector3(x, y, z);

            const paperData = {
                type: 'paper',
                spawnPosition: paperPos.clone(),
                label: paper.name,
                category: paper.category,
                clusterCenter: paperPos.clone(), // Paper is its own cluster center now
                info: paper.fullName,
                originalColorHex: paperNodeColor,
                url: paper.url
            };
            paperNodesData.push(paperData);
            paperPositions.push(paperPos.clone()); // Store position for team targeting

            // Group papers by category
            if (!paperNodeMap.has(paper.category)) {
                paperNodeMap.set(paper.category, []);
            }
            paperNodeMap.get(paper.category).push(paperData); // Store the whole data object
        });

        // 2. Calculate Category Centroids
        paperNodeMap.forEach((papers, categoryName) => {
            const centroid = new THREE.Vector3(0, 0, 0);
            papers.forEach(p => centroid.add(p.spawnPosition));
            centroid.divideScalar(papers.length);
            categoryCentroids.set(categoryName, centroid);
        });


        // 3. Identify Used Categories and Filter/Place Concepts near Centroids
        const usedCategories = new Set(allPapersInput.map(p => p.category));
        for (const categoryName in categories) {
            if (usedCategories.has(categoryName)) {
                const category = categories[categoryName];
                const centroid = categoryCentroids.get(categoryName); // Get the calculated centroid

                if (centroid) { // Check if centroid exists
                    category.labels.forEach(label => {
                        const info = `${category.infoPrefix} ${label}. Part of ${categoryName.toLowerCase().replace('/', ' and ')}.`;
                        const sentence = `${category.sentencePrefix} ${label.toLowerCase()}.`;
                        const originalConceptColor = conceptColorPalette[conceptColorIndex % conceptColorPalette.length];
                        conceptColorIndex++;

                        conceptNodesData.push({
                            type: 'concept',
                            // Spawn near the category centroid
                            spawnPosition: randomPosition(centroid.x, centroid.y, centroid.z, conceptNodeSpread),
                            label: label,
                            category: categoryName,
                            clusterCenter: centroid.clone(), // Associate with the category centroid
                            info: info,
                            originalColorHex: originalConceptColor,
                            sentence: sentence
                        });
                    });
                }
            }
        }

        // 4. Process Team Nodes
        const teamSpawnCenter = new THREE.Vector3(0, 0, 0); // Spawn team near origin
        teamMembersInput.forEach(member => {
            // Pick a random paper position as the initial target
            const initialTargetIndex = Math.floor(Math.random() * paperPositions.length);
            const initialTarget = paperPositions[initialTargetIndex] || teamSpawnCenter; // Fallback to origin if no papers

            teamNodesData.push({
                type: 'team',
                spawnPosition: randomPosition(teamSpawnCenter.x, teamSpawnCenter.y, teamSpawnCenter.z, teamNodeSpread),
                label: member.name,
                category: "Team",
                clusterCenter: teamSpawnCenter.clone(), // Team's conceptual center is origin
                info: member.name,
                originalColorHex: teamNodeColor,
                url: member.url,
                targetClusterCenter: initialTarget.clone(), // Target a paper position
                isMovingToTarget: true,
                pauseEndTime: 0
            });
        });


        // Combine all node data
        const allNodesData = [...paperNodesData, ...conceptNodesData, ...teamNodesData];

        // --- Three.js Setup Variables & DOM Refs ---
        let scene, camera, renderer, controls;
        let raycaster, mouse;
        let nodes = []; // All node meshes
        let teamNodes = []; // Just the team node meshes
        let otherNodes = []; // Concept and paper node meshes
        let paperConnectionLines = []; // ADDED: Lines connecting papers
        let stars; // Starfield points object
        let intersectedObject = null; // Currently hovered/targeted node
        let animationFrameId = null; // ID for cancelling animation loop
        let nodeBeingClicked = null; // Node currently undergoing click animation/overlay transition
        let clickEffectStartTime = 0; // Timestamp for click animation start
        const clock = new THREE.Clock(); // For delta time calculation
        let isMobile = false; // Flag for mobile device detection
        let lastTouchStartTime = 0; // ADDED: For mobile click prevention

        // DOM References
        const infoBox = document.getElementById('info-box');
        const infoLabel = document.getElementById('info-label');
        const infoDescription = document.getElementById('info-description');
        const canvas = document.getElementById('representation-canvas');
        const mainVisualizationDiv = document.getElementById('main-visualization');
        const animationOverlay = document.getElementById('animation-overlay');
        const overlayNodeLabel = document.getElementById('overlay-node-label');
        const overlayContent = document.getElementById('overlay-content');
        const signalWaveContainer = document.getElementById('signal-wave');
        const wordSelectionVis = document.getElementById('word-selection-vis');
        const overlayOutputContainer = document.getElementById('overlay-output-container');
        const overlayOutput = document.getElementById('overlay-output');
        const closeAnimationOverlayBtn = document.getElementById('close-animation-overlay-btn');
        const networkLayerDivs = overlayContent.querySelectorAll('.network-layer');
        const disclaimerOverlay = document.getElementById('disclaimer-overlay');
        const showDisclaimerLink = document.getElementById('show-disclaimer-link');
        const closeDisclaimerOverlayBtn = document.getElementById('close-disclaimer-overlay-btn');
        const teamListUl = document.getElementById('team-list');
        const mobileReticle = document.getElementById('mobile-reticle');
        const desktopInstructions = document.getElementById('desktop-instructions');
        const mobileInstructions = document.getElementById('mobile-instructions');


        // --- Mobile detection logic ---
        function detectMobile() {
            isMobile = window.innerWidth <= 768 && ('ontouchstart' in window || navigator.maxTouchPoints > 0);
            if (mobileReticle) {
                 mobileReticle.style.display = isMobile ? 'block' : 'none';
            }
             if (desktopInstructions && mobileInstructions) {
                 desktopInstructions.style.display = isMobile ? 'none' : 'inline';
                 mobileInstructions.style.display = isMobile ? 'inline' : 'none';
             }
             if (controls) {
                 controls.enablePan = !isMobile; // Disable panning on mobile (interferes with touch rotate)
             }
        }


        // --- Initialization ---
        function init() {
            detectMobile(); // Initial check

            // Scene setup
            scene = new THREE.Scene();
            scene.background = new THREE.Color(backgroundColor);
            scene.fog = new THREE.Fog(fogColor, fogNear, fogFar);

            // Camera setup
            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.z = 50; // Start further back due to spherical distribution

            // Renderer setup
            renderer = new THREE.WebGLRenderer({ canvas: canvas, antialias: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio); // Adjust for high DPI displays

            // Lighting
            const ambientLight = new THREE.AmbientLight(0xaaaaaa); // Soft ambient light
            scene.add(ambientLight);
            const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8); // Brighter directional light
            directionalLight.position.set(10, 15, 10);
            scene.add(directionalLight);

            // Controls setup
            controls = new OrbitControls(camera, renderer.domElement);
            controls.enableDamping = true; // Smooth camera movement
            controls.dampingFactor = 0.05;
            controls.screenSpacePanning = false; // Panning moves parallel to camera plane
            controls.minDistance = 5; // Allow slightly closer zoom
            controls.maxDistance = 120; // Prevent zooming too far out
            controls.enablePan = !isMobile; // Disable pan initially if mobile
            // --- Allow full vertical rotation ---
            controls.minPolarAngle = 0; // radians
            controls.maxPolarAngle = Math.PI; // radians


            // Raycaster setup (for mouse/reticle intersection)
            raycaster = new THREE.Raycaster();
            mouse = new THREE.Vector2(); // Stores normalized mouse coordinates (-1 to +1)
            raycaster.params.Points.threshold = 2.0;
            raycaster.params.Line.threshold = 1.0;

            // --- Create Nodes ---
            // Geometries are shared
            const conceptGeometry = new THREE.SphereGeometry(conceptNodeSize, 16, 8);
            const paperGeometry = new THREE.SphereGeometry(paperNodeSize, 12, 6);
            const teamGeometry = new THREE.SphereGeometry(teamNodeSize, 14, 7);

            // Create Meshes from processed data
            allNodesData.forEach(data => {
                let geometry; let initialColor; let nodeMesh;
                switch (data.type) {
                    case 'paper': geometry = paperGeometry; initialColor = defaultSpaceColor; break;
                    case 'team': geometry = teamGeometry; initialColor = teamNodeColor; break;
                    case 'concept': default: geometry = conceptGeometry; initialColor = defaultSpaceColor; break;
                }
                const material = new THREE.MeshLambertMaterial({ color: initialColor });
                nodeMesh = new THREE.Mesh(geometry, material);
                nodeMesh.position.copy(data.spawnPosition); // Use pre-calculated spawn position
                nodeMesh.userData = {
                    ...data, baseScale: 1.0, animOffset: Math.random() * Math.PI * 2,
                    originalColor: new THREE.Color(data.originalColorHex),
                    spaceColor: new THREE.Color(defaultSpaceColor),
                    targetColor: new THREE.Color(initialColor), isNode: true,
                };
                data.mesh = nodeMesh; // Store mesh reference back in data
                scene.add(nodeMesh);
                nodes.push(nodeMesh); // Add to the list of all nodes
                if (data.type === 'team') { teamNodes.push(nodeMesh); }
                else { otherNodes.push(nodeMesh); } // Concepts and Papers
            });

            // --- ADDED: Create Paper Connection Lines ---
            const lineMaterialInactive = new THREE.LineBasicMaterial({
                color: lineDefaultColor, transparent: true, opacity: lineDefaultOpacity, depthWrite: false
            });
            // Connect papers that share the same category (or connect all papers?)
            // Let's connect all papers within a certain distance for now to reduce clutter
            const maxLineDistanceSq = 30 * 30; // Max distance squared for connecting papers
            for (let i = 0; i < paperNodesData.length; i++) {
                for (let j = i + 1; j < paperNodesData.length; j++) {
                    const p1 = paperNodesData[i].spawnPosition;
                    const p2 = paperNodesData[j].spawnPosition;
                    if (p1.distanceToSquared(p2) < maxLineDistanceSq) {
                         const points = [p1, p2];
                         const geometry = new THREE.BufferGeometry().setFromPoints(points);
                         const lineMaterial = lineMaterialInactive.clone();
                         const line = new THREE.Line(geometry, lineMaterial);
                         line.userData = {
                              startPoint: p1.clone(), // Store positions for activation check
                              endPoint: p2.clone(),
                              targetOpacity: lineDefaultOpacity,
                              targetColor: new THREE.Color(lineDefaultColor)
                          };
                         scene.add(line);
                         paperConnectionLines.push(line);
                    }
                }
            }

            // --- Create Starfield ---
            const starVertices = [];
            for (let i = 0; i < starCount; i++) {
                const x = THREE.MathUtils.randFloatSpread(starSphereRadius * 2);
                const y = THREE.MathUtils.randFloatSpread(starSphereRadius * 2);
                const z = THREE.MathUtils.randFloatSpread(starSphereRadius * 2);
                const vec = new THREE.Vector3(x, y, z);
                if (vec.length() > starSphereRadius) { vec.setLength(starSphereRadius * THREE.MathUtils.randFloat(0.8, 1.0)); }
                else if (vec.length() < 50) { vec.setLength(50 + Math.random() * 50); }
                starVertices.push(vec.x, vec.y, vec.z);
            }
            const starGeometry = new THREE.BufferGeometry();
            starGeometry.setAttribute('position', new THREE.Float32BufferAttribute(starVertices, 3));
            const starMaterial = new THREE.PointsMaterial({
                color: 0xaaaaaa, size: 0.3, sizeAttenuation: true,
                transparent: true, opacity: 0.7, depthWrite: false
            });
            stars = new THREE.Points(starGeometry, starMaterial);
            scene.add(stars);

            // --- Populate Team List in Disclaimer Modal ---
            teamListUl.innerHTML = '';
            teamMembersInput.forEach(member => {
                const li = document.createElement('li'); const a = document.createElement('a');
                a.href = member.url; a.textContent = member.name;
                a.target = "_blank"; a.rel = "noopener noreferrer";
                li.appendChild(a); teamListUl.appendChild(li);
            });

            // --- Create signal dot elements for LLM overlay ---
            signalWaveContainer.innerHTML = '';
            for (let i = 0; i < numSignalDots; i++) {
                const dot = document.createElement('div'); dot.classList.add('signal-dot-item');
                dot.style.top = `${(i / (numSignalDots - 1)) * 100 - 50}%`;
                dot.style.transform = `translateY(-50%)`;
                signalWaveContainer.appendChild(dot);
            }

            // --- Event Listeners ---
            window.addEventListener('resize', onWindowResize, false);
            window.addEventListener('mousemove', onMouseMove, false);
            canvas.addEventListener('click', onCanvasClick, false);
            // --- ADDED: Record touch start time for mobile click prevention ---
            canvas.addEventListener('touchstart', (event) => {
                if (isMobile) {
                    // Only record time if it's a new touch sequence (e.g., first finger down)
                    if (event.touches.length === 1) {
                         lastTouchStartTime = Date.now();
                    }
                }
            }, { passive: true }); // Use passive: true if not calling preventDefault() inside
            // --- Prevent context menu on canvas (mobile hold) ---
            canvas.addEventListener('contextmenu', (event) => event.preventDefault());
            closeAnimationOverlayBtn.addEventListener('click', hideAnimationOverlay, false);
            showDisclaimerLink.addEventListener('click', showDisclaimerOverlay, false);
            closeDisclaimerOverlayBtn.addEventListener('click', hideDisclaimerOverlay, false);

            startAnimationLoop(); // Begin the main animation loop
        }

        // --- Animation Loop Control ---
        function startAnimationLoop() { if (!animationFrameId) { animate(); } }
        function stopAnimationLoop() { if (animationFrameId) { cancelAnimationFrame(animationFrameId); animationFrameId = null; } }

        // --- Event Handlers ---
        function onWindowResize() {
             // Update camera aspect ratio and projection matrix
             camera.aspect = window.innerWidth / window.innerHeight;
             camera.updateProjectionMatrix();
             // Resize renderer
             renderer.setSize(window.innerWidth, window.innerHeight);
             // Re-check mobile status as window size might change device type perception
             detectMobile();
         }

        function onMouseMove(event) {
             // Update mouse coordinates only if on desktop (mobile uses reticle)
             if (!isMobile) {
                 mouse.x = (event.clientX / window.innerWidth) * 2 - 1;
                 mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;
             }
         }

        function onCanvasClick(event) {
             // --- ADDED: Mobile Click Prevention ---
             if (isMobile && (Date.now() - lastTouchStartTime < mobileClickTimeThreshold)) {
                 // console.log("Mobile click ignored (too fast after touchstart)");
                 return; // Ignore click if too soon after touchstart
             }

            // Ignore clicks if an overlay is open or a node is already being processed for click
            if (animationOverlay.style.display === 'flex' || nodeBeingClicked || disclaimerOverlay.style.display === 'flex') return;

            let clickedNodeObject = null;

            if (isMobile) {
                // Mobile selection uses the intersectedObject determined by checkIntersections
                clickedNodeObject = intersectedObject;
            } else {
                // Desktop uses raycasting
                mouse.x = (event.clientX / window.innerWidth) * 2 - 1;
                mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;
                raycaster.setFromCamera(mouse, camera);
                const intersects = raycaster.intersectObjects(nodes);
                if (intersects.length > 0 && intersects[0].object.userData.isNode) {
                    clickedNodeObject = intersects[0].object;
                }
            }

            if (clickedNodeObject) {
                const nodeData = clickedNodeObject.userData;
                nodeBeingClicked = clickedNodeObject; // Mark this node as the one being processed
                clickEffectStartTime = Date.now(); // Start visual effect timer

                // Handle different click actions
                if ((nodeData.type === 'paper' || nodeData.type === 'team') && nodeData.url) {
                    // Open URL after a short delay (allows click effect to start)
                    setTimeout(() => {
                        if(nodeBeingClicked === clickedNodeObject) window.open(nodeData.url, '_blank');
                        // Reset click state for non-concept nodes after URL open attempt
                         if (nodeData.type !== 'concept' && nodeBeingClicked === clickedNodeObject) {
                              // Allow click effect to end naturally in animate loop
                         }
                    }, 150);
                } else if (nodeData.type === 'concept' && nodeData.sentence) {
                    // Delay showing the overlay slightly to allow click effect to be seen
                    setTimeout(() => {
                        if (nodeBeingClicked === clickedNodeObject) {
                            showAnimationOverlay(nodeData.label, nodeData.sentence);
                            // Overlay opening will handle resetting nodeBeingClicked when closed
                        }
                    }, transitionDelay);
                } else {
                     // If it's a node without a URL or sentence
                     // Allow click effect to end naturally in animate loop
                }
            }
        }


        // --- Overlay Controls ---
        let animationControl = { timeoutId: null, wordIndex: 0, words: [], isCancelled: false };

        function showAnimationOverlay(label, sentence) {
            if (disclaimerOverlay.style.display === 'flex') return;
            stopAnimationLoop(); // Pause the main 3D animation
            mainVisualizationDiv.style.display = 'none';
            overlayNodeLabel.textContent = `Exploring Concept: ${label}`;
            animationOverlay.style.display = 'flex';
            clearTimeout(animationControl.timeoutId);
            animationControl.isCancelled = false;

            // Reset animation elements
            signalWaveContainer.style.transition = 'none'; signalWaveContainer.style.opacity = '0'; signalWaveContainer.style.left = '-30px';
            overlayOutput.innerHTML = ''; wordSelectionVis.innerHTML = ''; wordSelectionVis.classList.remove('visible');
            networkLayerDivs.forEach(layer => layer.classList.remove('active'));
            void signalWaveContainer.offsetWidth; // Force reflow

            // Start the LLM animation process
            animationControl.words = sentence.split(' ');
            animationControl.wordIndex = 0;
            animateLLMStep();
         }

        function hideAnimationOverlay() {
             animationControl.isCancelled = true; clearTimeout(animationControl.timeoutId);
             animationOverlay.style.display = 'none';
             mainVisualizationDiv.style.display = 'block';
             // *** ADDED: Call onWindowResize after making div visible ***
             onWindowResize();
             // Reset click state when overlay closes
             nodeBeingClicked = null;
             startAnimationLoop();
         }

        function showDisclaimerOverlay(event) {
            event.preventDefault();
            // Prevent opening if animation overlay is open or a node is being clicked
            if (animationOverlay.style.display === 'flex' || nodeBeingClicked) return;
            stopAnimationLoop(); mainVisualizationDiv.style.display = 'none'; disclaimerOverlay.style.display = 'flex';
         }

        function hideDisclaimerOverlay() {
            disclaimerOverlay.style.display = 'none';
            mainVisualizationDiv.style.display = 'block';
            // *** ADDED: Call onWindowResize after making div visible ***
            onWindowResize();
            // Reset click state if disclaimer is closed
            nodeBeingClicked = null;
            startAnimationLoop();
         }

        // --- FASTER LLM Animation Step --- (Unchanged)
        async function animateLLMStep() {
             if (animationControl.isCancelled) return;
             if (animationControl.wordIndex >= animationControl.words.length) {
                 wordSelectionVis.innerHTML = `<span class="prob-word selected">Done.</span>`; wordSelectionVis.classList.add('visible');
                 animationControl.timeoutId = setTimeout(() => { if (!animationControl.isCancelled) wordSelectionVis.classList.remove('visible'); }, 750); return;
             }
             const currentWord = animationControl.words[animationControl.wordIndex]; const stepDuration = layerPassDuration / (numNetworkLayers + 1);
             signalWaveContainer.style.transition = 'none'; signalWaveContainer.style.left = '-30px'; void signalWaveContainer.offsetWidth; signalWaveContainer.style.opacity = '1';
             signalWaveContainer.style.transition = `left ${stepDuration}ms linear, opacity 0.1s linear`;
             for (let currentLayer = 1; currentLayer <= numNetworkLayers + 1; currentLayer++) {
                 if (animationControl.isCancelled) return; const targetX = ( (currentLayer -1) * (overlayContent.offsetWidth / numNetworkLayers)) ; signalWaveContainer.style.left = `${targetX}px`;
                 if (currentLayer > 1 && currentLayer <= numNetworkLayers + 1) networkLayerDivs[currentLayer - 2].classList.remove('active');
                 if (currentLayer > 0 && currentLayer <= numNetworkLayers) networkLayerDivs[currentLayer - 1].classList.add('active'); await delay(stepDuration);
             }
             if (!animationControl.isCancelled && numNetworkLayers > 0) networkLayerDivs[numNetworkLayers - 1].classList.remove('active');
             signalWaveContainer.style.opacity = '0'; if (animationControl.isCancelled) return; wordSelectionVis.innerHTML = '';
             genericAlternatives.forEach(altWord => { if (altWord.toLowerCase() !== currentWord.toLowerCase()) { const altSpan = document.createElement('span'); altSpan.textContent = altWord; altSpan.classList.add('prob-word', 'alternative'); wordSelectionVis.appendChild(altSpan); } });
             const selectedSpan = document.createElement('span'); selectedSpan.textContent = currentWord; selectedSpan.classList.add('prob-word', 'selected');
             const insertIndex = Math.floor(Math.random() * (wordSelectionVis.children.length + 1)); wordSelectionVis.insertBefore(selectedSpan, wordSelectionVis.children[insertIndex]);
             wordSelectionVis.classList.add('visible'); await delay(wordSelectionDuration); if (animationControl.isCancelled) return; wordSelectionVis.classList.remove('visible');
             const span = document.createElement('span'); span.textContent = currentWord + ' '; span.classList.add('current-word'); overlayOutput.appendChild(span);
             animationControl.wordIndex++; if (!animationControl.isCancelled) animationControl.timeoutId = setTimeout(animateLLMStep, wordAppendDelay);
         }

        // --- Core Logic: Intersection Checks ---
        // Uses screen-space check (closest to edge) for mobile, raycaster for desktop
        function checkIntersections() {
            let newIntersectedObject = null; // Node that becomes the intersectedObject

            if (isMobile) {
                // --- Mobile: Screen-Space Check (Closest to Edge) ---
                const reticleRadiusPixels = 96 / 2;
                const reticleRadiusNDC = reticleRadiusPixels / (window.innerHeight / 2);
                const reticleRadiusNDCSq = reticleRadiusNDC * reticleRadiusNDC;

                let bestNode = null;
                let minDistToEdge = reticleRadiusNDC + 0.01; // Find node closest to edge

                const nodeWorldPos = new THREE.Vector3(); // Reuse vector for performance
                const nodeScreenPos = new THREE.Vector3(); // Reuse vector for performance

                nodes.forEach(node => {
                    node.getWorldPosition(nodeWorldPos); // Get world position
                    nodeScreenPos.copy(nodeWorldPos).project(camera); // Project to screen space (NDC)

                    // Basic check: Is the node roughly within the viewport and in front of the camera?
                    if (nodeScreenPos.z > -1 && nodeScreenPos.z < 1 &&
                        nodeScreenPos.x > -1.1 && nodeScreenPos.x < 1.1 && // Allow slight margin
                        nodeScreenPos.y > -1.1 && nodeScreenPos.y < 1.1)
                    {
                        // Calculate distance squared from screen center (0,0) to the node's projected (x,y)
                        const screenDistSq = nodeScreenPos.x * nodeScreenPos.x + nodeScreenPos.y * nodeScreenPos.y;

                        // Check if node center is within the reticle radius squared
                        if (screenDistSq < reticleRadiusNDCSq) {
                             const screenDist = Math.sqrt(screenDistSq);
                             // Calculate how close the node's center is to the edge of the circle
                             const distToEdge = Math.abs(screenDist - reticleRadiusNDC);

                             // If this node is closer to the edge than the current best, select it
                             if (distToEdge < minDistToEdge) {
                                 minDistToEdge = distToEdge;
                                 bestNode = node;
                             }
                        }
                    }
                });
                newIntersectedObject = bestNode; // The node whose center is closest to the reticle edge

            } else {
                // --- Desktop: Raycaster Check ---
                raycaster.setFromCamera(mouse, camera);
                const intersects = raycaster.intersectObjects(nodes);
                if (intersects.length > 0 && intersects[0].object.userData.isNode) {
                    newIntersectedObject = intersects[0].object;
                }
            }

            // Update reticle style if mobile
            if (isMobile && mobileReticle) {
                 if (newIntersectedObject) {
                     mobileReticle.classList.add('target-locked');
                 } else {
                     mobileReticle.classList.remove('target-locked');
                 }
             }

            // Update info box only if the intersected object has changed
            if (intersectedObject !== newIntersectedObject) {
                intersectedObject = newIntersectedObject;

                if (intersectedObject) {
                    const data = intersectedObject.userData;
                    infoLabel.textContent = data.label;
                    infoDescription.textContent = data.info;
                    infoBox.className = 'hover-' + data.type;
                    infoBox.style.display = 'block';
                } else {
                    infoBox.style.display = 'none';
                    infoBox.className = '';
                }
            }
         } // End checkIntersections


        // --- Main Animation Loop ---
        function animate() {
            animationFrameId = requestAnimationFrame(animate);
            const delta = clock.getDelta();
            const elapsed = clock.getElapsedTime();
            const now = Date.now();

            // Update controls (handles damping, user input)
            controls.update();

            checkIntersections(); // Update intersected object and reticle style

            // Rotate starfield
            if (stars) { stars.rotation.y += delta * 0.01; stars.rotation.x += delta * 0.005; }

            const cameraPosition = camera.position; // Read camera position AFTER controls.update()
            const pausedPaperTargets = new Map(); // Track how many probes are paused at each paper node

            // --- Team Node Update Loop ---
            teamNodes.forEach(teamNode => {
                const teamData = teamNode.userData;
                let isSelected = (teamNode === nodeBeingClicked); // Check if this node is the selected one

                // --- Team Node Movement & Pause Logic ---
                // Pause movement ONLY if selected (clicked)
                if (!isSelected) {
                    // Only move if not selected
                    if (!teamData.isMovingToTarget) {
                        // Node is PAUSED (at a paper node position)
                        // Use the target position vector directly for the map key (requires exact match)
                        // Alternatively, convert to string, but direct object reference might be okay if vectors aren't cloned elsewhere
                        const targetKey = teamData.targetClusterCenter; // Use the vector object itself as key (careful!)
                        // Or safer: const targetKey = teamData.targetClusterCenter.toArray().toString();
                        pausedPaperTargets.set(targetKey, (pausedPaperTargets.get(targetKey) || 0) + 1);

                        // Check if pause duration is over
                        if (now > teamData.pauseEndTime) {
                            // *** Check selection state before resuming ***
                            if (!isSelected) {
                                // Select a new, different target paper position
                                let newTargetIndex;
                                let currentTargetIndex = paperPositions.findIndex(pos => vectorsAreEqual(pos, teamData.targetClusterCenter));
                                if (paperPositions.length > 1) {
                                    do {
                                        newTargetIndex = Math.floor(Math.random() * paperPositions.length);
                                    } while (newTargetIndex === currentTargetIndex);
                                } else {
                                    newTargetIndex = 0; // Only one paper, target it again
                                }

                                if (paperPositions[newTargetIndex]) {
                                     teamData.targetClusterCenter.copy(paperPositions[newTargetIndex]);
                                     teamData.isMovingToTarget = true; // Start moving again
                                } else {
                                    // Fallback if something went wrong finding paper positions
                                    teamData.isMovingToTarget = false;
                                    teamData.pauseEndTime = now + teamPauseDuration; // Stay paused
                                }

                            } else {
                                // If selected when pause ends, reset the timer to stay paused
                                teamData.pauseEndTime = now + teamPauseDuration;
                            }
                        }
                    } else {
                        // Node is MOVING towards a paper position
                        const distanceToTarget = teamNode.position.distanceTo(teamData.targetClusterCenter);
                        if (distanceToTarget <= teamTargetReachedThreshold) {
                            teamData.isMovingToTarget = false;
                            teamData.pauseEndTime = now + teamPauseDuration;
                        } else {
                            teamNode.position.lerp(teamData.targetClusterCenter, teamMoveSpeed);
                        }
                    }
                } // End if(!isSelected) block for movement

                // --- Team Node Scaling (Visual effect, happens even if selected/paused) ---
                const teamOscillation = Math.sin(elapsed * baseAnimFreq * 1.1 + teamData.animOffset) * baseAnimAmpPaperTeam;
                let teamTargetScale = teamData.baseScale + teamOscillation;
                const teamDistance = cameraPosition.distanceTo(teamNode.position);
                const teamDistanceFactor = Math.max(0, 1.0 - teamDistance / distanceScaleFactor);
                const teamDistanceBonus = maxDistanceScaleBonus * teamDistanceFactor * teamDistanceFactor;
                teamTargetScale *= (1.0 + teamDistanceBonus);

                // Apply hover scale (if not selected)
                if (!isSelected && teamNode === intersectedObject) {
                     teamTargetScale = 1.3;
                 }
                 // Apply click pulse scale (if selected)
                 if (isSelected) {
                     let clickScale = 1.0;
                     if (now < clickEffectStartTime + clickEffectDuration) {
                         const progress = (now - clickEffectStartTime) / clickEffectDuration;
                         clickScale = 1.0 + Math.sin(progress * Math.PI) * 0.3; // Smaller pulse
                     } else {
                         // Reset click state ONLY if not a concept node (concept node reset is handled by overlay close)
                         if (teamData.type !== 'concept') {
                              nodeBeingClicked = null;
                         }
                     }
                     teamTargetScale = clickScale; // Click scale overrides others
                 }

                teamNode.scale.lerp(new THREE.Vector3(teamTargetScale, teamTargetScale, teamTargetScale), 0.1);
                teamNode.material.color.setHex(teamNodeColor);
            });


            // --- ADDED: Paper Connection Line Activation Logic ---
             paperConnectionLines.forEach(line => {
                 const lineData = line.userData;
                 let pausedProbesAtEnds = 0;
                 // Check if any team node is paused near either endpoint of this line
                 pausedPaperTargets.forEach((count, targetPosition) => {
                      if (vectorsAreEqual(targetPosition, lineData.startPoint) || vectorsAreEqual(targetPosition, lineData.endPoint)) {
                           pausedProbesAtEnds += count;
                      }
                 });

                 let targetOpacity;
                 if (!lineData.targetColor) lineData.targetColor = new THREE.Color(); // Initialize if needed

                 if (pausedProbesAtEnds > 0) {
                     // Activate line: White color, increased opacity
                     targetOpacity = Math.min(lineMaxActiveOpacity, lineDefaultOpacity + lineOpacityIncreasePerProbe * pausedProbesAtEnds);
                     lineData.targetColor.setHex(lineActiveColor);
                 } else {
                     // Deactivate line: Default gray color, default opacity
                     targetOpacity = lineDefaultOpacity;
                     lineData.targetColor.setHex(lineDefaultColor);
                 }
                 lineData.targetOpacity = targetOpacity; // Store target for lerping

                 // Smoothly transition opacity and color
                 line.material.opacity = THREE.MathUtils.lerp(line.material.opacity, lineData.targetOpacity, lineTransitionSpeed);
                 line.material.color.lerp(lineData.targetColor, lineTransitionSpeed);
             });


            // --- Proximity Color Change & Other Node Updates (Concepts & Papers) ---
            const activatedNodes = new Set();
            teamNodes.forEach(teamNode => {
                 // Check proximity against papers and concepts
                 otherNodes.forEach(otherNode => {
                      if (teamNode.position.distanceTo(otherNode.position) < proximityThreshold) {
                          activatedNodes.add(otherNode);
                      }
                 });
            });

            otherNodes.forEach(node => { // Includes papers and filtered concepts
                const nodeData = node.userData;
                let isSelected = (node === nodeBeingClicked); // Check if this node is selected
                let targetScale = nodeData.baseScale || 1.0;
                let finalTargetColor = nodeData.spaceColor.clone(); // Start gray

                // Activate color if near a team node
                if (activatedNodes.has(node)) {
                    finalTargetColor = nodeData.originalColor.clone();
                }

                // --- Node Movement (Wiggle) ---
                // Pause movement ONLY if selected (clicked)
                if (!isSelected) {
                    // Only move if not selected
                    const moveOffset = new THREE.Vector3(
                        Math.sin(elapsed * nodeMoveFreq * 0.8 + nodeData.animOffset) * nodeMoveAmplitude,
                        Math.cos(elapsed * nodeMoveFreq * 1.2 + nodeData.animOffset) * nodeMoveAmplitude,
                        Math.sin(elapsed * nodeMoveFreq + nodeData.animOffset * 0.5) * nodeMoveAmplitude
                    );
                    // Wiggle around spawn position
                    const anchorPos = nodeData.spawnPosition;
                    const targetPosition = anchorPos.clone().add(moveOffset);
                    node.position.lerp(targetPosition, nodeLerpFactor);
                }

                // --- Node Scaling & Color (Applied even if selected/paused) ---
                const animAmp = (nodeData.type === 'concept') ? baseAnimAmpConcept : baseAnimAmpPaperTeam;
                const oscillation = Math.sin(elapsed * baseAnimFreq + nodeData.animOffset) * animAmp;
                targetScale += oscillation;
                const distance = cameraPosition.distanceTo(node.position);
                const distanceFactor = Math.max(0, 1.0 - distance / distanceScaleFactor);
                const distanceBonus = maxDistanceScaleBonus * distanceFactor * distanceFactor;
                targetScale *= (1.0 + distanceBonus);

                // Click Effect (if selected)
                if (isSelected) {
                    // Use specific click color only for concepts, otherwise just pulse
                    finalTargetColor = (nodeData.type === 'concept') ? new THREE.Color(clickHighlightColor) : nodeData.originalColor.clone();

                    let clickScale = 1.0;
                    if (now < clickEffectStartTime + clickEffectDuration) {
                        const progress = (now - clickEffectStartTime) / clickEffectDuration;
                        const pulseFactor = (nodeData.type === 'concept') ? 0.5 : 0.3; // Bigger pulse for concepts
                        clickScale = 1.0 + Math.sin(progress * Math.PI) * pulseFactor;
                    } else {
                         // Reset click state ONLY if not a concept node (concept node reset is handled by overlay close)
                         if (nodeData.type !== 'concept') {
                              nodeBeingClicked = null;
                         }
                    }
                    targetScale = clickScale; // Apply pulse scale
                }
                // Hover Effect (if not selected, but hovered)
                else if (node === intersectedObject) {
                    targetScale = 1.3; // Enlarge on hover/target
                    // Apply specific highlight color based on type (overrides proximity color)
                    if (nodeData.type === 'concept') finalTargetColor = new THREE.Color(highlightColor);
                    else if (nodeData.type === 'paper') finalTargetColor = new THREE.Color(paperHighlightColor);
                    // Team node highlighting is handled in their own loop
                }

                node.material.color.lerp(finalTargetColor, colorLerpFactor);
                node.scale.lerp(new THREE.Vector3(targetScale, targetScale, targetScale), 0.1);
            });

            // Render the scene
            renderer.render(scene, camera);
        }


        // --- Initialize ---
        window.onload = init;

    </script>
</body>
</html>
