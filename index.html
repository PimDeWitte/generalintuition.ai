<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>General Intuition - Research Lab</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">

    <style>
        /* CSS remains largely the same */
        body {
            margin: 0;
            overflow: hidden;
            font-family: 'Inter', sans-serif;
            color: #eee;
            background-color: #020210; /* Ensure background is set */
        }
        canvas { display: block; }
        #info-box, #instructions {
            position: absolute;
            background-color: rgba(0, 0, 0, 0.8);
            border-radius: 8px;
            padding: 10px 15px;
            font-size: 13px;
            z-index: 10;
            line-height: 1.6;
        }
        #info-box {
            top: 20px;
            left: 20px;
            border: 1px solid #0f0; /* Default highlight */
            color: #eee; /* Default text color */
            max-width: 320px;
            display: none;
            pointer-events: none;
            text-shadow: none; /* Remove default text shadow */
        }
        #info-box h3 {
            margin-top: 0;
            margin-bottom: 8px;
            font-size: 15px;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: #eee; /* Default header color */
            text-shadow: none;
        }
        #info-box.hover-concept h3 { color: #0f0; text-shadow: 0 0 3px #0f0; }
        #info-box.hover-paper h3 { color: #0ff; text-shadow: 0 0 3px #0ff; }
        #info-box.hover-team h3 { color: #ffd700; text-shadow: 0 0 3px #ffd700; }
        #info-box.hover-concept { border-color: #0f0; }
        #info-box.hover-paper { border-color: #0ff; }
        #info-box.hover-team { border-color: #ffd700; }


        #instructions {
            bottom: 15px;
            left: 50%;
            transform: translateX(-50%);
            color: #aaa;
            font-size: 11px;
            padding: 6px 12px;
            text-align: center;
        }
        /* --- Mobile Reticle --- */
        #mobile-reticle {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 24px; /* Size of the target */
            height: 24px; /* Size of the target */
            border: 2px solid rgba(255, 255, 255, 0.6); /* White, semi-transparent */
            border-radius: 50%;
            pointer-events: none; /* Doesn't interfere with other interactions */
            display: none; /* Hidden by default, shown via JS for mobile */
            z-index: 50; /* Above canvas, below overlays */
            box-sizing: border-box;
        }
        /* Style for the inner dot of the reticle */
         #mobile-reticle::after {
             content: '';
             position: absolute;
             top: 50%;
             left: 50%;
             transform: translate(-50%, -50%);
             width: 4px;
             height: 4px;
             background-color: rgba(255, 255, 255, 0.6);
             border-radius: 50%;
         }


        #instructions a {
            color: #0ff;
            text-decoration: none;
            display: block;
            margin-top: 6px;
            cursor: pointer;
            font-size: 12px;
        }
        #instructions a:hover { text-decoration: underline; }

        /* --- Overlay Styles (Unchanged) --- */
        #animation-overlay, #disclaimer-overlay {
            position: fixed; top: 0; left: 0; width: 100%; height: 100%;
            display: none; justify-content: center; align-items: center;
            z-index: 100; padding: 20px; box-sizing: border-box;
            font-family: 'Inter', sans-serif;
        }
        #animation-overlay { background-color: rgba(0, 0, 0, 0.9); flex-direction: column; }
        #disclaimer-overlay { background-color: rgba(10, 10, 20, 0.95); overflow-y: auto; }

        #overlay-node-label { color: #bbb; font-size: 16px; margin-bottom: 20px; text-align: center; font-weight: 700; }
        #overlay-content { display: flex; align-items: center; justify-content: space-around; width: 90%; max-width: 700px; height: 80px; margin-bottom: 25px; position: relative; border-bottom: 1px dashed rgba(0, 255, 255, 0.3); padding-bottom: 20px; }
        .network-layer { width: 8px; height: 100%; background: linear-gradient(to bottom, rgba(0, 100, 100, 0.3), rgba(0, 200, 200, 0.6), rgba(0, 100, 100, 0.3)); border-radius: 4px; box-shadow: 0 0 6px rgba(0, 255, 255, 0.4); opacity: 0; animation: fadeInLayer 0.3s forwards; transition: box-shadow 0.1s linear, background 0.1s linear; position: relative; overflow: hidden; }
        .network-layer::before { content: ''; position: absolute; top: 0; left: 0; width: 100%; height: 100%; background: repeating-linear-gradient( 45deg, rgba(255,255,255,0.0), rgba(255,255,255,0.0) 2px, rgba(255,255,255,0.05) 2px, rgba(255,255,255,0.05) 4px ); opacity: 0; transition: opacity 0.1s linear; }
        .network-layer:nth-child(1) { animation-delay: 0.02s; } .network-layer:nth-child(2) { animation-delay: 0.04s; } .network-layer:nth-child(3) { animation-delay: 0.06s; } .network-layer:nth-child(4) { animation-delay: 0.08s; } .network-layer:nth-child(5) { animation-delay: 0.1s; }
        .network-layer.active { box-shadow: 0 0 15px #0f0; background: linear-gradient(to bottom, rgba(0, 255, 0, 0.3), rgba(100, 255, 100, 1), rgba(0, 255, 0, 0.3)); }
        .network-layer.active::before { opacity: 0.5; }
        #signal-wave { position: absolute; left: -30px; top: 50%; height: 20px; transform: translateY(-50%); opacity: 0; }
        .signal-dot-item { position: absolute; width: 5px; height: 5px; background-color: #0f0; border-radius: 50%; box-shadow: 0 0 5px #0f0; }
        #word-selection-vis { position: absolute; bottom: -15px; left: 50%; transform: translateX(-50%); width: auto; min-width: 120px; padding: 4px; background-color: rgba(0, 0, 0, 0.7); border: 1px solid #444; border-radius: 4px; text-align: center; opacity: 0; transition: opacity 0.1s ease-out; pointer-events: none; display: flex; justify-content: center; align-items: center; gap: 6px; }
        #word-selection-vis.visible { opacity: 1; }
        .prob-word { font-size: 11px; padding: 2px 5px; border-radius: 3px; }
        .prob-word.selected { color: #0f0; border: 1px solid #0f0; background-color: rgba(0, 255, 0, 0.2); font-weight: 700; }
        .prob-word.alternative { color: #888; border: 1px solid #444; background-color: rgba(80, 80, 80, 0.2); opacity: 0.7; }
        #overlay-output-container { width: 90%; max-width: 700px; min-height: 60px; text-align: left; border: 1px dashed #0ff; padding: 12px; border-radius: 5px; background-color: rgba(0, 50, 50, 0.3); margin-top: 15px; }
        #overlay-output { font-size: 15px; color: #0f0; text-shadow: 0 0 5px #0f0; line-height: 1.7; }
        #overlay-output .current-word { background-color: rgba(0, 255, 0, 0.3); padding: 0 2px; border-radius: 2px; animation: fadeHighlight 0.5s forwards; }
        #overlay-explanation { color: #0ff; font-size: 12px; margin-top: 20px; max-width: 600px; text-align: center; line-height: 1.6; text-decoration: underline; }
        #overlay-explanation a { color: #0ff; text-decoration: none;}
        #overlay-explanation a:hover { color: #3ff; }

        .close-overlay-btn {
            position: absolute; top: 15px; right: 15px; background-color: #dd0000; color: white; border: none; padding: 6px 12px; border-radius: 5px; cursor: pointer;
            font-family: 'Inter', sans-serif; font-size: 13px; font-weight: 700;
            box-shadow: 0 0 8px #ff0000; transition: background-color 0.2s;
        }
        .close-overlay-btn:hover { background-color: #aa0000; }

        /* --- Disclaimer Overlay Styles (Updated Content Area) --- */
        #disclaimer-content {
            background-color: rgba(0, 0, 0, 0.6); border: 1px solid #0ff; border-radius: 8px;
            padding: 25px 35px; max-width: 650px; /* Adjusted max-width */
            width: 90%; color: #ccc; font-size: 14px; line-height: 1.8;
            font-family: 'Inter', sans-serif;
            text-align: center; /* Center content */
         }
        #disclaimer-content h2 {
             font-family: 'Press Start 2P', cursive;
            color: #0ff; font-size: 20px; margin-bottom: 25px; text-align: center; text-shadow: 0 0 5px #0ff; letter-spacing: 1px;
        }
        #disclaimer-content h3 {
            font-family: 'Press Start 2P', cursive;
            color: #eee; font-size: 15px; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px dashed #555; padding-bottom: 8px; letter-spacing: 0.5px;
            display: inline-block; /* Keep border only under text */
            width: auto; /* Adjust width */
        }
        #disclaimer-content p, #disclaimer-content li { margin-bottom: 12px; }
        #disclaimer-content strong { color: #fff; font-weight: 700; }
        #disclaimer-content ul { list-style: none; padding-left: 0; margin-left: 0; }
        #team-list {
            margin-bottom: 30px; /* Add space below team list */
            display: inline-block; /* Center the list block */
            text-align: left; /* Align list items left */
        }
        #team-list li { margin-bottom: 8px; }
        #team-list li a {
            color: #ffd700;
            text-decoration: none;
            font-weight: 700;
            font-size: 14px;
        }
        #team-list li a:hover { text-decoration: underline; color: #fff; }
        /* Style for the email link */
        #disclaimer-content .email-link {
            color: #0ff;
            text-decoration: none;
            font-size: 14px;
            margin-top: 15px;
            display: block; /* Make it block to center easily */
        }
         #disclaimer-content .email-link:hover {
            text-decoration: underline;
            color: #3ff;
        }


        /* --- Animations (Unchanged) --- */
        @keyframes fadeInLayer { from { opacity: 0; transform: scaleY(0.5); } to { opacity: 1; transform: scaleY(1); } }
        @keyframes nodeClickFlash { 0%, 100% { transform: scale(1.0); } 50% { transform: scale(1.5); } }
        @keyframes fadeHighlight { from { background-color: rgba(0, 255, 0, 0.3); } to { background-color: transparent; } }
    </style>
</head>
<body class="font-sans">
    <div id="main-visualization">
        <div id="info-box"><h3 id="info-label">...</h3><p id="info-description">...</p></div>
        <div id="mobile-reticle"></div>
        <div id="instructions">
             <span id="desktop-instructions">Click & Drag: Rotate | Scroll: Zoom | Hover: Inspect</span>
             <span id="mobile-instructions" style="display: none;">Drag: Rotate | Pinch: Zoom | Center Reticle: Inspect</span>
            <a id="show-disclaimer-link" href="#">About / Info</a>
        </div>
        <canvas id="representation-canvas"></canvas>
    </div>

    <div id="animation-overlay">
         <div id="overlay-node-label">Exploring Concept: ...</div>
        <div id="overlay-content">
            <div class="network-layer"></div> <div class="network-layer"></div> <div class="network-layer"></div> <div class="network-layer"></div> <div class="network-layer"></div>
            <div id="signal-wave"></div>
             <div id="word-selection-vis"></div>
        </div>
        <div id="overlay-output-container">
            <p id="overlay-output"></p>
        </div>
         <div id="overlay-explanation">
             careers@generalintuition.ai
         </div>
        <button id="close-animation-overlay-btn" class="close-overlay-btn">X</button>
    </div>

    <div id="disclaimer-overlay">
         <div id="disclaimer-content">
            <h2>General Intuition</h2>

            <ul id="team-list">

            </ul>

            <a href="mailto:careers@generalintuition.ai" class="email-link">careers@generalintuition.ai</a>

        </div>
        <button id="close-disclaimer-overlay-btn" class="close-overlay-btn">X</button>
    </div>

    <script type="importmap">
        { "imports": { "three": "https://cdn.jsdelivr.net/npm/three@0.163.0/build/three.module.js", "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.163.0/examples/jsm/" } }
    </script>

    <script type="module">
        import * as THREE from 'three';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        // --- Configuration ---
        const conceptNodeSize = 0.7;
        const paperNodeSize = 0.55;
        const teamNodeSize = 0.6;
        const paperNodeColor = 0x00ffff;
        const teamNodeColor = 0xffd700;
        const highlightColor = 0x00ff00;
        const paperHighlightColor = 0x00ffff;
        const teamHighlightColor = 0xffd700;
        const clickHighlightColor = 0xffffff;
        const defaultSpaceColor = 0xcccccc;
        const numNetworkLayers = 5;
        const clickEffectDuration = 500;
        const transitionDelay = 300;
        const conceptNodeSpread = 10;
        const paperNodeSpread = 6;
        const teamNodeSpread = 5;

        // Node Movement Params
        const nodeMoveAmplitude = 0.3;
        const nodeMoveFreq = 0.4;
        const nodeLerpFactor = 0.02;

        // Team Probe Movement Params
        const teamMoveSpeed = 0.008;
        const teamTargetReachedThreshold = 3.0;
        const teamPauseDuration = 2500;

        // Proximity Color Change Params
        const proximityThreshold = 5.0;
        const colorLerpFactor = 0.08;

        // Cluster Line Params
        const lineDefaultColor = 0x444444;
        const lineDefaultOpacity = 0.1;
        // *** MODIFIED: Changed active line color to white ***
        const lineActiveColor = 0xffffff;
        const lineMaxActiveOpacity = 0.35;
        const lineOpacityIncreasePerProbe = 0.06;
        const lineTransitionSpeed = 0.1;

        // Background Params
        const backgroundColor = 0x020210;
        const fogColor = backgroundColor;
        const fogNear = 80;
        const fogFar = 200;
        const starCount = 7000;
        const starSphereRadius = 400;

        // Animation Params
        const baseAnimFreq = 0.8;
        const baseAnimAmpConcept = 0.03;
        const baseAnimAmpPaperTeam = 0.05;
        const distanceScaleFactor = 15.0;
        const maxDistanceScaleBonus = 0.4;

        // Faster Animation Timings
        const layerPassDuration = 240;
        const wordSelectionDuration = 90;
        const wordAppendDelay = 20;
        const numSignalDots = 5;

        // Concept Color Palette
        const conceptColorPalette = [
            0x4a90e2, 0xbd10e0, 0xaaaaaa, 0x7ed321, 0xf5a623, 0x50e3c2, 0xe04457,
        ];
        let conceptColorIndex = 0;

        // --- Helper Functions ---
        function randomPosition(baseX, baseY, baseZ, spread) {
            return new THREE.Vector3(
                baseX + (Math.random() * spread - spread / 2),
                baseY + (Math.random() * spread - spread / 2),
                baseZ + (Math.random() * spread - spread / 2)
            );
        }
        const genericAlternatives = ["concept", "process", "state"];
        const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
        function vectorsAreEqual(v1, v2, tolerance = 0.01) {
            return v1.distanceTo(v2) < tolerance;
        }


        // --- Node Data ---
        const conceptNodesData = [];
        const paperNodesData = [];
        const teamNodesData = [];

       const categories = {
             "Perception": { labels: ["Sensor Fusion", "Proprioception", "Visual Odometry", "SLAM", "Object Recognition", "Depth Sensing", "Tactile Sensing", "Auditory Scene"], base: [25, 25, 0], infoPrefix: "Sensing:", sentencePrefix: "Making sense of the world involves techniques like Sensor Fusion, Proprioception, and Visual Odometry." },
             "Action/Control": { labels: ["Reinforcement Learning", "Inverse Kinematics", "PID Control", "Motion Planning", "Grasping", "Locomotion", "Trajectory Optimization", "Imitation Learning"], base: [-25, 25, 0], infoPrefix: "Acting:", sentencePrefix: "Generating physical behavior relies on methods such as Reinforcement Learning, Inverse Kinematics, and Motion Planning." },
             "Cognition/Planning": { labels: ["Task Planning", "World Model", "Predictive Coding", "Affordance Theory", "Symbolic Reasoning", "Goal Setting", "Decision Making", "Memory Systems"], base: [25, -25, 0], infoPrefix: "Thinking:", sentencePrefix: "Formulating plans for interaction involves concepts like Task Planning, World Models, and Symbolic Reasoning." },
             "Embodiment": { labels: ["Morphology", "Physical Interaction", "Degrees of Freedom", "Actuators", "Simulation Gap", "Embodied Cognition", "Soft Robotics", "Bio-inspiration"], base: [-25, -25, 0], infoPrefix: "Form:", sentencePrefix: "The physical form and its interaction capabilities are defined by aspects such as Morphology, Degrees of Freedom, and Actuators." },
             "Core AI/ML": { labels: ["Neural Network", "Transformer", "Attention Mechanism", "Backpropagation", "Gradient Descent", "Latent Space", "CNN", "RNN", "GAN"], base: [0, 0, 25], infoPrefix: "Algorithms:", sentencePrefix: "Extracting patterns from data often utilizes core AI/ML techniques like Neural Networks, Transformers, and Backpropagation." },
             "Learning Paradigms": { labels: ["Supervised", "Unsupervised", "Self-Supervised", "Continual Learning", "Meta-Learning", "Transfer Learning", "Active Learning", "Online Learning"], base: [0, 0, -25], infoPrefix: "Adaptation:", sentencePrefix: "Improving capabilities often involves different learning paradigms such as Supervised, Unsupervised, and Self-Supervised learning." },
             "Human-Robot Interaction": { labels: ["Social Robotics", "Trust in Automation", "Shared Autonomy", "Teleoperation", "Gesture Recognition", "Natural Language Interface", "Explainable AI (XAI)"], base: [0, 25, 25], infoPrefix: "Collaboration:", sentencePrefix: "Facilitating human-robot collaboration relies on concepts like Social Robotics, Trust in Automation, and Shared Autonomy." },
             "Robotic Manipulation": { labels: ["Dexterous Manipulation", "Force Control", "Assembly Tasks", "In-Hand Manipulation", "Regrasping", "Compliant Control", "Peg-in-Hole"], base: [0, -25, 25], infoPrefix: "Handling:", sentencePrefix: "Physically engaging with objects requires capabilities such as Dexterous Manipulation, Force Control, and Assembly Task execution." },
             "Navigation & Mapping": { labels: ["Path Planning", "Global Localization", "Topological Maps", "Grid Maps", "Exploration Algorithms", "Multi-Agent Path Finding", "Semantic Mapping"], base: [25, 0, 25], infoPrefix: "Movement:", sentencePrefix: "Moving effectively through environments depends on techniques like Path Planning, Global Localization, and various Mapping strategies." },
             "Computer Vision": { labels: ["Image Segmentation", "Pose Estimation", "Scene Understanding", "3D Reconstruction", "Optical Flow", "Feature Detection", "Video Analysis"], base: [-25, 0, 25], infoPrefix: "Seeing:", sentencePrefix: "Interpreting visual input often involves Computer Vision techniques such as Image Segmentation, Pose Estimation, and Scene Understanding." },
             "Natural Language Processing": { labels: ["Language Models (LLMs)", "Sentiment Analysis", "Machine Translation", "Question Answering", "Text Summarization", "Named Entity Recognition", "Embeddings"], base: [25, 25, -25], infoPrefix: "Language:", sentencePrefix: "Grounding communication often involves Natural Language Processing methods like Language Models (LLMs), Sentiment Analysis, and Machine Translation." },
             "Knowledge Representation": { labels: ["Ontologies", "Knowledge Graphs", "Logical Inference", "Semantic Web", "Rule-Based Systems", "Description Logics", "Common Sense Reasoning"], base: [-25, 25, -25], infoPrefix: "Knowledge:", sentencePrefix: "Structuring world knowledge utilizes methods such as Ontologies, Knowledge Graphs, and Logical Inference." },
             "Optimization Methods": { labels: ["Gradient-Based Opt.", "Evolutionary Algorithms", "Bayesian Optimization", "Convex Optimization", "Integer Programming", "Search Algorithms (A*, BFS)"], base: [25, -25, -25], infoPrefix: "Optimizing:", sentencePrefix: "Finding optimal solutions or sequences often involves Optimization Methods like Gradient-Based Optimization, Evolutionary Algorithms, and Search Algorithms." },
             "Simulation & Modeling": { labels: ["Physics Engines", "Digital Twins", "System Identification", "Agent-Based Modeling", "Finite Element Analysis", "Sim-to-Real Transfer"], base: [-25, -25, -25], infoPrefix: "Modeling:", sentencePrefix: "Anticipating system behavior and consequences often uses Simulation & Modeling tools like Physics Engines, Digital Twins, and System Identification." },
             "Cognitive Architectures": { labels: ["SOAR", "ACT-R", "Connectionism", "Symbolic Systems", "Hybrid Architectures", "Developmental Robotics", "Modular Architectures"], base: [0, 25, -25], infoPrefix: "Mind Models:", sentencePrefix: "Integrating diverse cognitive capabilities often relies on Cognitive Architectures such as SOAR, ACT-R, and Hybrid Systems." },
             "Neuroscience Inspiration": { labels: ["Spiking Neural Nets", "Hebbian Learning", "Hippocampal Models", "Cortical Columns", "Neural Plasticity", "Motor Cortex Models"], base: [0, -25, -25], infoPrefix: "Brain-Inspired:", sentencePrefix: "Drawing inspiration from biology involves studying mechanisms like Spiking Neural Networks, Hebbian Learning, and Neural Plasticity." },
             "Philosophy of Mind": { labels: ["Consciousness", "Qualia", "Intentionality", "Free Will", "Mind-Body Problem", "Embodied Mind Thesis", "Extended Mind"], base: [25, 0, -25], infoPrefix: "Mind Concepts:", sentencePrefix: "Exploring fundamental questions about mind and agency involves concepts like Consciousness, Intentionality, and the Mind-Body Problem." },
             "Ethics & Safety": { labels: ["AI Alignment", "Value Learning", "Bias Detection", "Fairness Metrics", "Robustness", "Interpretability", "Accountability"], base: [-25, 0, -25], infoPrefix: "Safety:", sentencePrefix: "Ensuring safe, reliable, and ethical AI behavior involves addressing areas like AI Alignment, Value Learning, Bias Detection, and Robustness." },
             "Multi-Agent Systems": { labels: ["Coordination", "Cooperation", "Competition", "Swarm Intelligence", "Distributed Control", "Negotiation"], base: [15, 15, 15], infoPrefix: "Groups:", sentencePrefix: "Coordinating multiple interacting agents requires techniques for Coordination, Cooperation, Competition, and Swarm Intelligence." },
             "Probabilistic Methods": { labels: ["Bayesian Networks", "Kalman Filters", "Particle Filters", "Gaussian Processes", "Markov Models (HMMs)", "Probabilistic Programming"], base: [-15, 15, 15], infoPrefix: "Uncertainty:", sentencePrefix: "Handling uncertainty and noisy data often involves Probabilistic Methods like Bayesian Networks, Kalman Filters, and Markov Models." },
             "Geometric Methods": { labels: ["Lie Groups (SE3)", "Manifold Learning", "Computational Geometry", "Topology", "Differential Geometry", "Configuration Space"], base: [15, -15, 15], infoPrefix: "Geometry:", sentencePrefix: "Reasoning about spatial relationships, configurations, and transformations often uses Geometric Methods including Lie Groups, Manifold Learning, and Computational Geometry." },
             "Information Theory": { labels: ["Entropy", "Mutual Information", "KL Divergence", "Compression", "Channel Capacity", "Coding Theory"], base: [-15, -15, 15], infoPrefix: "Information:", sentencePrefix: "Quantifying information, uncertainty, and communication efficiency uses concepts from Information Theory such as Entropy, Mutual Information, and KL Divergence." },
             "Control Theory": { labels: ["Linear Control", "Nonlinear Control", "Adaptive Control", "Optimal Control", "Robust Control", "Stochastic Control"], base: [15, 15, -15], infoPrefix: "Dynamics:", sentencePrefix: "Achieving stable and desired system behavior often requires principles from Control Theory, including Linear Control, Adaptive Control, and Optimal Control." },
             "Hardware & Sensors": { labels: ["LIDAR", "IMU", "Cameras", "Force Sensors", "Encoders", "Microcontrollers", "FPGAs", "GPUs"], base: [-15, 15, -15], infoPrefix: "Hardware:", sentencePrefix: "The physical interface for robotic systems includes Hardware & Sensors like LIDAR, IMUs, Cameras, and processing units such as GPUs." },
             "Data Structures & Algo": { labels: ["Graphs", "Trees", "Hash Tables", "Sorting", "Dynamic Programming", "Complexity Theory"], base: [15, -15, -15], infoPrefix: "Computation:", sentencePrefix: "Efficient computation and data management relies on fundamental Data Structures & Algorithms like Graphs, Trees, Sorting, and Dynamic Programming." },
             "Quantum Computing": { labels: ["Qubits", "Superposition", "Entanglement", "Quantum Gates", "Quantum Algorithms", "Quantum ML"], base: [-15, -15, -15], infoPrefix: "Quantum:", sentencePrefix: "Exploring future computational paradigms involves understanding Quantum Computing concepts such as Qubits, Superposition, Entanglement, and Quantum Algorithms." },
             "Biologically Plausible Learning": { labels: ["Synaptic Plasticity", "Neuromodulation", "Homeostasis", "Local Learning Rules", "Spike-Timing-Dependent Plasticity (STDP)"], base: [30, 0, 0], infoPrefix: "Bio Learning:", sentencePrefix: "Developing learning mechanisms inspired by biology involves exploring concepts like Synaptic Plasticity, Neuromodulation, and Spike-Timing-Dependent Plasticity (STDP)." },
             "Developmental AI": { labels: ["Intrinsic Motivation", "Curiosity-Driven Learning", "Sensorimotor Stages", "Affordance Learning", "Skill Acquisition"], base: [-30, 0, 0], infoPrefix: "Growth:", sentencePrefix: "Acquiring skills progressively, akin to natural development, involves concepts like Intrinsic Motivation, Curiosity-Driven Learning, and Sensorimotor Stages." },
             "Causality": { labels: ["Causal Inference", "Structural Causal Models", "Do-Calculus", "Counterfactuals", "Intervention"], base: [0, 30, 0], infoPrefix: "Cause/Effect:", sentencePrefix: "Understanding cause-and-effect relationships and action outcomes requires reasoning about Causality using tools like Causal Inference, Structural Causal Models, and Counterfactuals." },
             "Theory of Computation": { labels: ["Turing Machines", "Computability", "Complexity Classes (P vs NP)", "Automata Theory", "Formal Languages"], base: [0, -30, 0], infoPrefix: "Computation Theory:", sentencePrefix: "Understanding the fundamental limits and capabilities of computation relates to the Theory of Computation, including concepts like Turing Machines, Computability, and Complexity Classes." },
              "Cybernetics": { labels: ["Feedback Loops", "Control Systems", "Communication Theory", "Self-Regulation", "Homeostasis (biological)"], base: [0, 0, 30], infoPrefix: "Systems:", sentencePrefix: "Understanding systems that maintain stability and achieve goals through feedback involves principles of Cybernetics, such as Feedback Loops, Control Systems, and Self-Regulation." },
              "Spacecraft Operations": { labels: ["Asteroid Navigation", "Orbital Docking", "Deep Space Probe", "Fleet Coordination", "Emergency Maneuvers", "Resource Scanning", "Interstellar Travel", "Lander Control"], base: [0, 15, 35], infoPrefix: "Space Ops:", sentencePrefix: "Autonomous control in space involves complex tasks like Asteroid Navigation, Orbital Docking, Deep Space Probe management, and Lander Control." },
              "Human Intuition": { labels: ["Gut Feeling", "Implicit Learning", "Heuristics", "Expert Judgment", "Analogical Reasoning", "Emotional Intelligence"], base: [35, 15, 15], infoPrefix: "Human Intuition:", sentencePrefix: "Understanding human cognition involves exploring phenomena like Gut Feelings, Implicit Learning, Heuristics, and Expert Judgment." },
              "Machine Intuition": { labels: ["Emergent Behavior", "Subsymbolic Reasoning", "Latent Representations", "Policy Gradients (RL)", "Generative Models", "Few-Shot Adaptation"], base: [-35, -15, -15], infoPrefix: "Machine Intuition:", sentencePrefix: "Developing AI capable of intuitive-like responses involves fostering Emergent Behavior, Subsymbolic Reasoning, and leveraging Latent Representations from models like Policy Gradients and Generative Models." },
             // Added a placeholder sentence for Team as labels are empty. Adjust if needed.
              "Team": { labels: [], base: [0, -15, -15], infoPrefix: "Team:", sentencePrefix: "Defining the collaborative structure and goals." }
         };
        const clusterCenters = {};
        const conceptClusterCenters = [];

        // Populate conceptNodesData
        for (const categoryName in categories) {
            const category = categories[categoryName];
            const [bx, by, bz] = category.base;
            const centerPoint = new THREE.Vector3(bx, by, bz);
            clusterCenters[categoryName] = centerPoint;
             if (categoryName !== "Team") conceptClusterCenters.push(centerPoint);
             if (categoryName === "Team") continue;
            category.labels.forEach(label => {
                const info = `${category.infoPrefix} ${label}. Part of ${categoryName.toLowerCase().replace('/', ' and ')}.`;
                const sentence = `${category.sentencePrefix} ${label.toLowerCase()}.`;
                const originalConceptColor = conceptColorPalette[conceptColorIndex % conceptColorPalette.length];
                conceptColorIndex++;
                conceptNodesData.push({ type: 'concept', spawnPosition: randomPosition(bx, by, bz, conceptNodeSpread), label: label, category: categoryName, clusterCenter: centerPoint, info: info, originalColorHex: originalConceptColor, sentence: sentence });
            });
        }

        // Paper Node Definitions
        const existingPapersInput = [ /* ... paper data ... */
            { name: "JARVIS-VLA", url: "https://x.com/_akhaliq/status/1903155708457013610?s=12&t=TGTk_lVC4Ugrh9Op2GBFbQ", category: "Human-Robot Interaction", fullName: "JARVIS-VLA: A Generalist Vision-Language-Action Model for Robotic Control" },
            { name: "Gemma3", url: "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf", category: "Natural Language Processing", fullName: "Gemma3 Technical Report" },
            { name: "Portal Agents", url: "https://zhongwen.one/projects/portal/", category: "Multi-Agent Systems", fullName: "Agents Play Thousands of 3D Video Games through Procedurally Generated Worlds (Portal)" },
            { name: "LLDMs", url: "https://arxiv.org/abs/2502.09992", category: "Core AI/ML", fullName: "Large Language Diffusion Models" },
            { name: "GRPO From Scratch", url: "https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb", category: "Optimization Methods", fullName: "Coding GRPO from Scratch: A Guide to Distributed Implementation" },
            { name: "Magma", url: "https://microsoft.github.io/Magma/", category: "Core AI/ML", fullName: "Magma: A Foundation Model for Multimodal AI Agents" },
            { name: "Helix", url: "https://www.figure.ai/news/helix", category: "Embodiment", fullName: "Helix: A Vision-Language-Action Model for Generalist Humanoid Control" },
            { name: "Scale on TPUs", url: "https://jax-ml.github.io/scaling-book/", category: "Hardware & Sensors", fullName: "How to Scale Your Model: A Systems View of LLMs on TPUs" },
            { name: "Ultra-Scale Playbook", url: "https://huggingface.co/spaces/nanotron/ultrascale-playbook", category: "Hardware & Sensors", fullName: "The Ultra-Scale Playbook: Training LLMs on GPU Clusters" },
            { name: "Gameplay Ideation", url: "https://www.nature.com/articles/s41586-025-08600-3", category: "Cognition/Planning", fullName: "World and Human Action Models towards gameplay ideation" },
            { name: "Value RL Scaling", url: "https://arxiv.org/abs/2502.04327", category: "Action/Control", fullName: "Value-Based Deep RL Scales Predictably" },
            { name: "Agent Scaling Laws", url: "https://arxiv.org/pdf/2411.04434", category: "Learning Paradigms", fullName: "Scaling Laws for Pre-training Agents and World Models" },
            { name: "NSA Attention", url: "https://x.com/deepseek_ai/status/1891745487071609327", category: "Core AI/ML", fullName: "NSA: A Hardware-Aligned and Natively Trainable Sparse Attention mechanism" },
            { name: "VideoJAM", url: "https://hila-chefer.github.io/videojam-paper.github.io/", category: "Computer Vision", fullName: "VideoJAM: Generating Videos with Motion Models and Subject Control" },
            { name: "OpenVLA", url: "https://openvla.github.io/", category: "Human-Robot Interaction", fullName: "OpenVLA: An Open-Source Vision-Language-Action Model" },
            { name: "π0 Policy", url: "https://www.physicalintelligence.company/blog/pi0", category: "Action/Control", fullName: "π0: Our First Generalist Policy" },
            { name: "Spotify: Jim Fan", url: "https://open.spotify.com/episode/2YEslWY161A5nAniNse3gR?si=vNtk6EVETxqNWPHCayo9ow&nd=1&dlsi=06144c246de64a1c", category: "Embodiment", fullName: "Spotify Jim Fan on Nvidia’s Embodied AI Lab and Jensen Huang’s Prediction" },
            { name: "Human-level FPS RL", url: "https://www.davidsilver.uk/wp-content/uploads/2020/03/ctf_compressed.pdf", category: "Multi-Agent Systems", fullName: "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning" },
            { name: "PaliGemma 2", url: "https://arxiv.org/pdf/2412.03555", category: "Core AI/ML", fullName: "PaliGemma 2: A Family of Versatile VLMs for Transfer" },
            { name: "Kimi k1.5", url: "https://github.com/MoonshotAI/Kimi-k1.5", category: "Action/Control", fullName: "Kimi k1.5: Scaling Reinforcement Learning with LLMs" },
            { name: "Compressed Video Action", url: "https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Compressed_Video_Action_CVPR_2018_paper.pdf", category: "Computer Vision", fullName: "Compressed Video Action Recognition" },
            { name: "SFT vs RL Generalization", url: "https://arxiv.org/pdf/2501.17161v1", category: "Learning Paradigms", fullName: "SFT Memorizes, RL Generalizes: A Comparative Study" },
            { name: "Cosmos World Model", url: "https://arxiv.org/pdf/2501.03575v1", category: "Cognition/Planning", fullName: "Cosmos World Foundation Model Platform for Physical AI" },
            { name: "GR00T N1", url: "https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T_1_Whitepaper.pdf", category: "Embodiment", fullName: "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots" },
            { name: "Genie Presentation", url: "https://www.youtube.com/watch?v=vs5a2JTy0K0", category: "Simulation & Modeling", fullName: "Genie UCL After dark presentation" },
            { name: "FAST Action Tokenization", url: "https://www.pi.website/research/fast", category: "Action/Control", fullName: "FAST: Efficient Action Tokenization for Vision-Language-Action Models" },
            { name: "Gemini Robotics", url: "https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/", category: "Embodiment", fullName: "Gemini Robotics: Bringing AI into the Physical World" },
            { name: "SIMA Agent", url: "https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/", category: "Multi-Agent Systems", fullName: "A generalist AI agent for 3D virtual environments (SIMA)" },
            { name: "Gaming, Goats & GI", url: "https://www.youtube.com/watch?v=64pndvbbokA", category: "Cognition/Planning", fullName: "Gaming, Goats & General Intelligence with Frederic Besse" },
            { name: "Mastering Control (World Models)", url: "https://arxiv.org/abs/2301.04104", category: "Cognition/Planning", fullName: "Mastering diverse control tasks through world models" },
            { name: "Pandora World Model", url: "https://world-model.maitrix.org/", category: "Cognition/Planning", fullName: "Pandora: Towards General World Model with Natural Language Actions and Video States" },
            { name: "Navigation World Models", url: "https://arxiv.org/pdf/2412.03572", category: "Navigation & Mapping", fullName: "Navigation World Models" },
            { name: "Seaweed-7B", url: "https://arxiv.org/abs/2504.08685", category: "Computer Vision", fullName: "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model" },
            { name: "Unified World Models", url: "https://weirdlabuw.github.io/uwm/", category: "Simulation & Modeling", fullName: "Unified World Models: Coupling Video and Action Diffusion" },
            { name: "UI-TARS", url: "https://seed-tars.com/1.5/", category: "Human-Robot Interaction", fullName: "UI-TARS: Pioneering Automated GUI Interaction with Native Agents" },
        ];
        const newPapersInput = [ /* ... paper data ... */
             { name: "Agent57", url: "https://arxiv.org/abs/2003.13350?utm_source=chatgpt.com", category: "Action/Control", fullName: "Agent57: Outperforming the Atari Human Benchmark" },
             { name: "Other-Play", url: "https://arxiv.org/pdf/2003.02979", category: "Multi-Agent Systems", fullName: "“Other-Play” for Zero-Shot Coordination" },
             { name: "NetHack Env", url: "https://arxiv.org/abs/2006.13760", category: "Simulation & Modeling", fullName: "The NetHack Learning Environment" },
             { name: "ReBeL", url: "https://arxiv.org/abs/2007.13544?utm_source=chatgpt.com", category: "Multi-Agent Systems", fullName: "Combining Deep Reinforcement Learning and Search for Imperfect-Information Games (ReBeL)" },
             { name: "XLand", url: "https://arxiv.org/abs/2107.12808?utm_source=chatgpt.com", category: "Developmental AI", fullName: "Open-Ended Learning Leads to Generally Capable Agents (XLand)" },
             { name: "VPT (New Link)", url: "https://arxiv.org/abs/2206.11795?utm_source=chatgpt.com", category: "Computer Vision", fullName: "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos" },
             { name: "MineDojo", url: "https://arxiv.org/pdf/2206.08853", category: "Simulation & Modeling", fullName: "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge" },
             { name: "DeepNash", url: "https://arxiv.org/abs/2206.15378?utm_source=chatgpt.com", category: "Multi-Agent Systems", fullName: "Mastering the Game of Stratego with Model-Free Multi-Agent RL (DeepNash)" },
             { name: "CICERO", url: "https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf?utm_source=chatgpt.com", category: "Multi-Agent Systems", fullName: "Human-Level Play in Diplomacy (CICERO)" },
             { name: "Voyager (New Link)", url: "https://arxiv.org/abs/2305.16291", category: "Embodiment", fullName: "Voyager: An Open-Ended Embodied Agent with Large Language Models" },
             { name: "Diffusion WM", url: "https://diamond-wm.github.io/", category: "Cognition/Planning", fullName: "Diffusion for World Modeling" },
        ];
        const allPapersInput = [...existingPapersInput, ...newPapersInput];

        // Populate paperNodesData
        allPapersInput.forEach(paper => {
            const targetCenter = clusterCenters[paper.category];
            if (targetCenter) {
                paperNodesData.push({ type: 'paper', spawnPosition: randomPosition(targetCenter.x, targetCenter.y, targetCenter.z, paperNodeSpread), label: paper.name, category: paper.category, clusterCenter: targetCenter, info: paper.fullName, originalColorHex: paperNodeColor, url: paper.url });
            } else { console.warn(`Category "${paper.category}" not found for paper "${paper.name}". Skipping node.`); }
        });

        // --- Team Member Definitions ---
        const teamMembersInput = [
            { name: "Eloi Alonso", url: "https://eloialonso.github.io/" },
            { name: "Adam Jelley", url: "https://adamjelley.github.io/" },
            { name: "Vincent Micheli", url: "https://vmicheli.github.io/" },
            { name: "Pim de Witte", url: "https://pimdewitte.com/" },
            { name: "Fredrik Noren", url: "https://www.linkedin.com/in/fredrik-noren/" },
            { name: "Florian Laurent", url: "https://masterscrat.github.io/" },
            { name: "Karthick Jeyapal", url: "https://www.linkedin.com/in/karthick-jeyapal-a9a4a524/" }
        ];

        // Populate teamNodesData
        const teamSpawnCenter = clusterCenters["Team"];
        if (teamSpawnCenter) {
            teamMembersInput.forEach(member => {
                const initialTargetIndex = Math.floor(Math.random() * conceptClusterCenters.length);
                const initialTarget = conceptClusterCenters[initialTargetIndex];
                teamNodesData.push({ type: 'team', spawnPosition: randomPosition(teamSpawnCenter.x, teamSpawnCenter.y, teamSpawnCenter.z, teamNodeSpread), label: member.name, category: "Team", clusterCenter: teamSpawnCenter, info: member.name, originalColorHex: teamNodeColor, url: member.url, targetClusterCenter: initialTarget.clone(), isMovingToTarget: true, pauseEndTime: 0 });
            });
        } else { console.warn("Team category center not found. Cannot place team nodes."); }

        // Combine all node data
        const allNodesData = [...conceptNodesData, ...paperNodesData, ...teamNodesData];

        // --- Three.js Setup Variables & DOM Refs ---
        let scene, camera, renderer, controls;
        let raycaster, mouse;
        let nodes = [];
        let teamNodes = [];
        let otherNodes = [];
        let clusterLines = [];
        let stars;
        let intersectedObject = null;
        let animationFrameId = null;
        let nodeBeingClicked = null;
        let clickEffectStartTime = 0;
        const clock = new THREE.Clock();
        // *** ADDED: Mobile detection flag ***
        let isMobile = false;
        // DOM References
        const infoBox = document.getElementById('info-box');
        const infoLabel = document.getElementById('info-label');
        const infoDescription = document.getElementById('info-description');
        const canvas = document.getElementById('representation-canvas');
        const mainVisualizationDiv = document.getElementById('main-visualization');
        const animationOverlay = document.getElementById('animation-overlay');
        const overlayNodeLabel = document.getElementById('overlay-node-label');
        const overlayContent = document.getElementById('overlay-content');
        const signalWaveContainer = document.getElementById('signal-wave');
        const wordSelectionVis = document.getElementById('word-selection-vis');
        const overlayOutputContainer = document.getElementById('overlay-output-container');
        const overlayOutput = document.getElementById('overlay-output');
        const closeAnimationOverlayBtn = document.getElementById('close-animation-overlay-btn');
        const networkLayerDivs = overlayContent.querySelectorAll('.network-layer');
        const disclaimerOverlay = document.getElementById('disclaimer-overlay');
        const showDisclaimerLink = document.getElementById('show-disclaimer-link');
        const closeDisclaimerOverlayBtn = document.getElementById('close-disclaimer-overlay-btn');
        const teamListUl = document.getElementById('team-list');
        // *** ADDED: Mobile reticle and instruction text refs ***
        const mobileReticle = document.getElementById('mobile-reticle');
        const desktopInstructions = document.getElementById('desktop-instructions');
        const mobileInstructions = document.getElementById('mobile-instructions');


        // *** ADDED: Mobile detection logic ***
        function detectMobile() {
            // Simple check: combination of screen width and touch capability
            isMobile = window.innerWidth <= 768 && ('ontouchstart' in window || navigator.maxTouchPoints > 0);

            // Show/hide the reticle and appropriate instructions based on detection
            if (mobileReticle) {
                 mobileReticle.style.display = isMobile ? 'block' : 'none';
            }
             if (desktopInstructions && mobileInstructions) {
                 desktopInstructions.style.display = isMobile ? 'none' : 'inline';
                 mobileInstructions.style.display = isMobile ? 'inline' : 'none';
             }
             // Update controls based on mobile status
             if (controls) {
                 controls.enablePan = !isMobile; // Disable panning on mobile maybe? Or keep it? User preference.
                 // OrbitControls often handles touch reasonably well, but you might
                 // want to adjust sensitivity or behavior for mobile here.
             }
        }


        // --- Initialization ---
        function init() {
            // *** ADDED: Initial mobile detection ***
            detectMobile();

            scene = new THREE.Scene();
            scene.background = new THREE.Color(backgroundColor);
            scene.fog = new THREE.Fog(fogColor, fogNear, fogFar);

            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.z = 50;

            renderer = new THREE.WebGLRenderer({ canvas: canvas, antialias: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);

            const ambientLight = new THREE.AmbientLight(0xaaaaaa);
            scene.add(ambientLight);
            const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
            directionalLight.position.set(10, 15, 10);
            scene.add(directionalLight);

            controls = new OrbitControls(camera, renderer.domElement);
            controls.enableDamping = true;
            controls.dampingFactor = 0.05;
            controls.screenSpacePanning = false; // Keep false for typical orbit behaviour
            controls.minDistance = 10;
            controls.maxDistance = 120;

            raycaster = new THREE.Raycaster();
            mouse = new THREE.Vector2();

            // *** MODIFIED: Increased threshold slightly for easier center selection ***
            raycaster.params.Points.threshold = 2.0;

            // Use node sizes from V4.1
            const conceptGeometry = new THREE.SphereGeometry(conceptNodeSize, 16, 8);
            const paperGeometry = new THREE.SphereGeometry(paperNodeSize, 12, 6);
            const teamGeometry = new THREE.SphereGeometry(teamNodeSize, 14, 7);

            // Create Nodes
            allNodesData.forEach(data => {
                let geometry; let initialColor; let nodeMesh;
                switch (data.type) {
                    case 'paper': geometry = paperGeometry; initialColor = defaultSpaceColor; break;
                    case 'team': geometry = teamGeometry; initialColor = teamNodeColor; break;
                    case 'concept': default: geometry = conceptGeometry; initialColor = defaultSpaceColor; break;
                }
                const material = new THREE.MeshLambertMaterial({ color: initialColor });
                nodeMesh = new THREE.Mesh(geometry, material);
                nodeMesh.position.copy(data.spawnPosition);
                nodeMesh.userData = {
                    ...data,
                    baseScale: 1.0,
                    animOffset: Math.random() * Math.PI * 2,
                    originalColor: new THREE.Color(data.originalColorHex),
                    spaceColor: new THREE.Color(defaultSpaceColor),
                    targetColor: new THREE.Color(initialColor),
                    isNode: true,
                    spawnPosition: data.spawnPosition.clone(),
                };
                scene.add(nodeMesh);
                nodes.push(nodeMesh);
                if (data.type === 'team') teamNodes.push(nodeMesh); else otherNodes.push(nodeMesh);
            });

            // Create Cluster Connection Lines
            const lineMaterialInactive = new THREE.LineBasicMaterial({ color: lineDefaultColor, transparent: true, opacity: lineDefaultOpacity, depthWrite: false });
            for (let i = 0; i < conceptClusterCenters.length; i++) {
                for (let j = i + 1; j < conceptClusterCenters.length; j++) {
                    const points = [conceptClusterCenters[i], conceptClusterCenters[j]];
                    const geometry = new THREE.BufferGeometry().setFromPoints(points);
                    const lineMaterial = lineMaterialInactive.clone();
                    const line = new THREE.Line(geometry, lineMaterial);
                    // Ensure targetColor is initialized correctly
                    line.userData = {
                         startPoint: conceptClusterCenters[i],
                         endPoint: conceptClusterCenters[j],
                         targetOpacity: lineDefaultOpacity,
                         targetColor: new THREE.Color(lineDefaultColor) // Initialize with default color object
                     };
                    scene.add(line);
                    clusterLines.push(line);
                }
            }

            // Create Starfield
            const starVertices = [];
            for (let i = 0; i < starCount; i++) {
                const x = THREE.MathUtils.randFloatSpread(starSphereRadius * 2);
                const y = THREE.MathUtils.randFloatSpread(starSphereRadius * 2);
                const z = THREE.MathUtils.randFloatSpread(starSphereRadius * 2);
                const vec = new THREE.Vector3(x, y, z);
                if (vec.length() > starSphereRadius) {
                    vec.setLength(starSphereRadius * THREE.MathUtils.randFloat(0.8, 1.0));
                } else if (vec.length() < 50) {
                     vec.setLength(50 + Math.random() * 50);
                }
                starVertices.push(vec.x, vec.y, vec.z);
            }
            const starGeometry = new THREE.BufferGeometry();
            starGeometry.setAttribute('position', new THREE.Float32BufferAttribute(starVertices, 3));
            const starMaterial = new THREE.PointsMaterial({
                color: 0xaaaaaa, size: 0.3, sizeAttenuation: true,
                transparent: true, opacity: 0.7, depthWrite: false
            });
            stars = new THREE.Points(starGeometry, starMaterial);
            scene.add(stars);

            // Populate Team List in Disclaimer Modal
            teamListUl.innerHTML = '';
            teamMembersInput.forEach(member => {
                const li = document.createElement('li'); const a = document.createElement('a');
                a.href = member.url; a.textContent = member.name;
                a.target = "_blank"; a.rel = "noopener noreferrer";
                li.appendChild(a); teamListUl.appendChild(li);
            });

            // Create signal dot elements
            signalWaveContainer.innerHTML = '';
            for (let i = 0; i < numSignalDots; i++) {
                const dot = document.createElement('div'); dot.classList.add('signal-dot-item');
                dot.style.top = `${(i / (numSignalDots - 1)) * 100 - 50}%`;
                dot.style.transform = `translateY(-50%)`;
                signalWaveContainer.appendChild(dot);
            }

            // Event Listeners
            window.addEventListener('resize', onWindowResize, false);
            // Only track mouse move if not on mobile for hover effects
            window.addEventListener('mousemove', onMouseMove, false);
            canvas.addEventListener('click', onCanvasClick, false);
             // Add touch listener for mobile interaction if needed, though OrbitControls handles basic touch drag/pinch
             // canvas.addEventListener('touchstart', onTouchStart, false); // Example if needed

            closeAnimationOverlayBtn.addEventListener('click', hideAnimationOverlay, false);
            showDisclaimerLink.addEventListener('click', showDisclaimerOverlay, false);
            closeDisclaimerOverlayBtn.addEventListener('click', hideDisclaimerOverlay, false);

            startAnimationLoop();
        }

        // --- Animation Loop Control ---
        function startAnimationLoop() { if (!animationFrameId) { animate(); } }
        function stopAnimationLoop() { if (animationFrameId) { cancelAnimationFrame(animationFrameId); animationFrameId = null; } }

        // --- Event Handlers ---
        function onWindowResize() {
             camera.aspect = window.innerWidth / window.innerHeight;
             camera.updateProjectionMatrix();
             renderer.setSize(window.innerWidth, window.innerHeight);
             // *** ADDED: Re-check mobile status on resize ***
             detectMobile();
         }

        function onMouseMove(event) {
             // Only update mouse coordinates if not in mobile mode (where hover is less relevant)
             if (!isMobile) {
                 mouse.x = (event.clientX / window.innerWidth) * 2 - 1;
                 mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;
             }
         }

         // *** MODIFIED: Handle click based on mobile/desktop ***
        function onCanvasClick(event) {
            if (animationOverlay.style.display === 'flex' || nodeBeingClicked || disclaimerOverlay.style.display === 'flex') return;

            let clickedNodeObject = null;

            if (isMobile) {
                // On mobile, the "click" targets whatever the center reticle is currently intersecting
                // This object is stored in `intersectedObject` by the continuous `checkIntersections`
                clickedNodeObject = intersectedObject;
            } else {
                // On desktop, use the actual mouse click position
                // Ensure mouse coordinates are updated (redundant if onMouseMove fired, but safe)
                mouse.x = (event.clientX / window.innerWidth) * 2 - 1;
                mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;
                raycaster.setFromCamera(mouse, camera);
                const intersects = raycaster.intersectObjects(nodes);
                if (intersects.length > 0 && intersects[0].object.userData.isNode) {
                    clickedNodeObject = intersects[0].object;
                }
            }

            // Perform action if a node was determined to be clicked (either by center reticle or mouse)
            if (clickedNodeObject) {
                const nodeData = clickedNodeObject.userData;
                if ((nodeData.type === 'paper' || nodeData.type === 'team') && nodeData.url) {
                    window.open(nodeData.url, '_blank');
                } else if (nodeData.type === 'concept' && nodeData.sentence) {
                    // Ensure we don't re-trigger if already animating/clicked
                    if(nodeBeingClicked !== clickedNodeObject) {
                        nodeBeingClicked = clickedNodeObject;
                        clickEffectStartTime = Date.now();
                        // Trigger visual click effect immediately if desired
                        // node.scale logic in animate() will handle the visual pulse

                        // Delay showing overlay slightly
                        setTimeout(() => {
                            // Check if the clicked node is still the one intended after delay
                            if (nodeBeingClicked === clickedNodeObject) {
                                showAnimationOverlay(nodeData.label, nodeData.sentence);
                            }
                        }, transitionDelay);
                    }
                }
            }
        }


        // --- Overlay Controls ---
        let animationControl = { timeoutId: null, wordIndex: 0, words: [], isCancelled: false };
        function showAnimationOverlay(label, sentence) {
            if (disclaimerOverlay.style.display === 'flex') return; stopAnimationLoop(); mainVisualizationDiv.style.display = 'none';
            overlayNodeLabel.textContent = `Exploring Concept: ${label}`; animationOverlay.style.display = 'flex'; clearTimeout(animationControl.timeoutId);
            animationControl.isCancelled = false; signalWaveContainer.style.transition = 'none'; signalWaveContainer.style.opacity = '0'; signalWaveContainer.style.left = '-30px';
            overlayOutput.innerHTML = ''; wordSelectionVis.innerHTML = ''; wordSelectionVis.classList.remove('visible');
            networkLayerDivs.forEach(layer => layer.classList.remove('active')); void signalWaveContainer.offsetWidth; animationControl.words = sentence.split(' ');
            animationControl.wordIndex = 0; animateLLMStep();
         }
        function hideAnimationOverlay() {
             animationControl.isCancelled = true; clearTimeout(animationControl.timeoutId); animationOverlay.style.display = 'none'; mainVisualizationDiv.style.display = 'block';
             nodeBeingClicked = null; // Reset clicked state when closing
             startAnimationLoop();
         }
        function showDisclaimerOverlay(event) {
            event.preventDefault(); if (animationOverlay.style.display === 'flex') return; stopAnimationLoop(); mainVisualizationDiv.style.display = 'none'; disclaimerOverlay.style.display = 'flex';
         }
        function hideDisclaimerOverlay() {
            disclaimerOverlay.style.display = 'none'; mainVisualizationDiv.style.display = 'block'; startAnimationLoop();
         }

        // --- FASTER LLM Animation Step ---
        async function animateLLMStep() {
             if (animationControl.isCancelled) return;
             if (animationControl.wordIndex >= animationControl.words.length) {
                 wordSelectionVis.innerHTML = `<span class="prob-word selected">Done.</span>`; wordSelectionVis.classList.add('visible');
                 animationControl.timeoutId = setTimeout(() => { if (!animationControl.isCancelled) wordSelectionVis.classList.remove('visible'); }, 750); return;
             }
             const currentWord = animationControl.words[animationControl.wordIndex]; const stepDuration = layerPassDuration / (numNetworkLayers + 1);
             signalWaveContainer.style.transition = 'none'; signalWaveContainer.style.left = '-30px'; void signalWaveContainer.offsetWidth; signalWaveContainer.style.opacity = '1';
             signalWaveContainer.style.transition = `left ${stepDuration}ms linear, opacity 0.1s linear`;
             for (let currentLayer = 1; currentLayer <= numNetworkLayers + 1; currentLayer++) {
                 if (animationControl.isCancelled) return; const targetX = ( (currentLayer -1) * (overlayContent.offsetWidth / numNetworkLayers)) ; signalWaveContainer.style.left = `${targetX}px`;
                 if (currentLayer > 1 && currentLayer <= numNetworkLayers + 1) networkLayerDivs[currentLayer - 2].classList.remove('active');
                 if (currentLayer > 0 && currentLayer <= numNetworkLayers) networkLayerDivs[currentLayer - 1].classList.add('active'); await delay(stepDuration);
             }
             if (!animationControl.isCancelled && numNetworkLayers > 0) networkLayerDivs[numNetworkLayers - 1].classList.remove('active');
             signalWaveContainer.style.opacity = '0'; if (animationControl.isCancelled) return; wordSelectionVis.innerHTML = '';
             genericAlternatives.forEach(altWord => { if (altWord.toLowerCase() !== currentWord.toLowerCase()) { const altSpan = document.createElement('span'); altSpan.textContent = altWord; altSpan.classList.add('prob-word', 'alternative'); wordSelectionVis.appendChild(altSpan); } });
             const selectedSpan = document.createElement('span'); selectedSpan.textContent = currentWord; selectedSpan.classList.add('prob-word', 'selected');
             const insertIndex = Math.floor(Math.random() * (wordSelectionVis.children.length + 1)); wordSelectionVis.insertBefore(selectedSpan, wordSelectionVis.children[insertIndex]);
             wordSelectionVis.classList.add('visible'); await delay(wordSelectionDuration); if (animationControl.isCancelled) return; wordSelectionVis.classList.remove('visible');
             const span = document.createElement('span'); span.textContent = currentWord + ' '; span.classList.add('current-word'); overlayOutput.appendChild(span);
             animationControl.wordIndex++; if (!animationControl.isCancelled) animationControl.timeoutId = setTimeout(animateLLMStep, wordAppendDelay);
         }

        // --- Core Logic: Intersection Checks ---
        // *** MODIFIED: Use center screen or mouse based on mobile status ***
        function checkIntersections() {
            let raycastOrigin;
            if (isMobile) {
                // On mobile, always raycast from the center of the screen
                raycastOrigin = new THREE.Vector2(0, 0);
            } else {
                // On desktop, use the current mouse position
                raycastOrigin = mouse;
            }

            raycaster.setFromCamera(raycastOrigin, camera);
            const intersects = raycaster.intersectObjects(nodes); // Check intersection with all nodes

            let newIntersectedObject = null;
            if (intersects.length > 0 && intersects[0].object.userData.isNode) {
                newIntersectedObject = intersects[0].object;
            }

            // Update info box only if the intersected object changes
            if (intersectedObject !== newIntersectedObject) {
                intersectedObject = newIntersectedObject; // Update the globally tracked intersected object

                if (intersectedObject) {
                    const data = intersectedObject.userData;
                    infoLabel.textContent = data.label;
                    infoDescription.textContent = data.info;
                    // Update info box style based on node type
                    infoBox.className = 'hover-' + data.type; // Removes old classes, adds new one
                    infoBox.style.display = 'block';
                } else {
                    // No intersection, hide the info box
                    infoBox.style.display = 'none';
                    infoBox.className = ''; // Clear specific hover styles
                }
            }
        }


        // --- Main Animation Loop ---
        function animate() {
            animationFrameId = requestAnimationFrame(animate);
            const delta = clock.getDelta();
            const elapsed = clock.getElapsedTime();
            const now = Date.now();

            controls.update(); // Update camera controls (rotation, zoom)

            // *** checkIntersections is called continuously to update `intersectedObject` ***
            checkIntersections(); // Determine which node is currently under the cursor/reticle

            // Rotate starfield
            if (stars) {
                stars.rotation.y += delta * 0.01;
                stars.rotation.x += delta * 0.005;
            }

            const cameraPosition = camera.position;
            const pausedClusterCenters = new Map();

            // Team Node Movement & Pause Logic
            teamNodes.forEach(teamNode => {
                const teamData = teamNode.userData;
                if (!teamData.isMovingToTarget) {
                    const centerKey = teamData.targetClusterCenter.toArray().toString();
                    pausedClusterCenters.set(centerKey, (pausedClusterCenters.get(centerKey) || 0) + 1);
                    if (now > teamData.pauseEndTime) {
                        let newTargetIndex; let currentTargetIndex = conceptClusterCenters.findIndex(center => vectorsAreEqual(center, teamData.targetClusterCenter));
                        do { newTargetIndex = Math.floor(Math.random() * conceptClusterCenters.length); } while (newTargetIndex === currentTargetIndex && conceptClusterCenters.length > 1);
                        teamData.targetClusterCenter.copy(conceptClusterCenters[newTargetIndex]);
                        teamData.isMovingToTarget = true;
                    }
                } else {
                    teamNode.position.lerp(teamData.targetClusterCenter, teamMoveSpeed);
                    if (teamNode.position.distanceTo(teamData.targetClusterCenter) < teamTargetReachedThreshold) {
                        teamData.isMovingToTarget = false; teamData.pauseEndTime = now + teamPauseDuration;
                        teamNode.position.copy(teamData.targetClusterCenter);
                    }
                }
                // Team Node Scaling
                const teamOscillation = Math.sin(elapsed * baseAnimFreq * 1.1 + teamData.animOffset) * baseAnimAmpPaperTeam;
                let teamTargetScale = teamData.baseScale + teamOscillation;
                const teamDistance = cameraPosition.distanceTo(teamNode.position); const teamDistanceFactor = Math.max(0, 1.0 - teamDistance / distanceScaleFactor);
                const teamDistanceBonus = maxDistanceScaleBonus * teamDistanceFactor * teamDistanceFactor; teamTargetScale *= (1.0 + teamDistanceBonus);
                // *** Highlight based on `intersectedObject` ***
                if (teamNode === intersectedObject) teamTargetScale = 1.3;
                teamNode.scale.lerp(new THREE.Vector3(teamTargetScale, teamTargetScale, teamTargetScale), 0.1);
            });

            // Cluster Line Activation Logic
             clusterLines.forEach(line => {
                 const lineData = line.userData;
                 let pausedProbesAtEnds = 0;
                 const startKey = lineData.startPoint.toArray().toString();
                 const endKey = lineData.endPoint.toArray().toString();
                 pausedProbesAtEnds += pausedClusterCenters.get(startKey) || 0;
                 pausedProbesAtEnds += pausedClusterCenters.get(endKey) || 0;

                 let targetOpacity;
                 // Ensure targetColor is a Color object before setting/lerping
                 if (!lineData.targetColor) lineData.targetColor = new THREE.Color();

                 if (pausedProbesAtEnds > 0) {
                     targetOpacity = Math.min(lineMaxActiveOpacity, lineDefaultOpacity + lineOpacityIncreasePerProbe * pausedProbesAtEnds);
                     // *** Use the modified lineActiveColor (white) ***
                     lineData.targetColor.setHex(lineActiveColor);
                 } else {
                     targetOpacity = lineDefaultOpacity;
                     lineData.targetColor.setHex(lineDefaultColor);
                 }
                 lineData.targetOpacity = targetOpacity;

                 line.material.opacity = THREE.MathUtils.lerp(line.material.opacity, lineData.targetOpacity, lineTransitionSpeed);
                 line.material.color.lerp(lineData.targetColor, lineTransitionSpeed);
             });


            // Proximity Color Change & Other Node Updates
            const activatedNodes = new Set();
            teamNodes.forEach(teamNode => { otherNodes.forEach(otherNode => { if (teamNode.position.distanceTo(otherNode.position) < proximityThreshold) activatedNodes.add(otherNode); }); });

            otherNodes.forEach(node => {
                const nodeData = node.userData; let targetScale = nodeData.baseScale || 1.0; let isClickEffectActive = false; let finalTargetColor = nodeData.spaceColor.clone(); // Start with space color

                if (activatedNodes.has(node)) finalTargetColor = nodeData.originalColor.clone(); // Use original if activated by proximity

                // Node Movement
                const moveOffset = new THREE.Vector3( Math.sin(elapsed * nodeMoveFreq * 0.8 + nodeData.animOffset) * nodeMoveAmplitude, Math.cos(elapsed * nodeMoveFreq * 1.2 + nodeData.animOffset) * nodeMoveAmplitude, Math.sin(elapsed * nodeMoveFreq + nodeData.animOffset * 0.5) * nodeMoveAmplitude );
                const targetPosition = nodeData.spawnPosition.clone().add(moveOffset); node.position.lerp(targetPosition, nodeLerpFactor);

                // Base Oscillation & Distance Scaling
                const animAmp = baseAnimAmpConcept; const oscillation = Math.sin(elapsed * baseAnimFreq + nodeData.animOffset) * animAmp; targetScale += oscillation;
                const distance = cameraPosition.distanceTo(node.position); const distanceFactor = Math.max(0, 1.0 - distance / distanceScaleFactor); const distanceBonus = maxDistanceScaleBonus * distanceFactor * distanceFactor; targetScale *= (1.0 + distanceBonus);

                // Click Effect (visual pulse)
                if (node === nodeBeingClicked && nodeData.type === 'concept') {
                    finalTargetColor = new THREE.Color(clickHighlightColor); // Override color during click pulse
                    isClickEffectActive = true; let clickScale = 1.0;
                    if (now < clickEffectStartTime + clickEffectDuration) {
                        const progress = (now - clickEffectStartTime) / clickEffectDuration;
                        clickScale = 1.0 + Math.sin(progress * Math.PI) * 0.5; // Pulse effect
                    } else {
                        // Reset click state only if overlay isn't showing
                        if(animationOverlay.style.display !== 'flex') {
                             nodeBeingClicked = null;
                        }
                        // Scale returns to normal after effect duration unless highlighted
                    }
                    targetScale = clickScale; // Apply click pulse scale
                }

                // Hover Effect (based on intersectedObject updated by checkIntersections)
                // Apply hover effect *after* proximity check and *if not* currently being clicked
                if (!isClickEffectActive && node === intersectedObject) {
                    targetScale = 1.3; // Enlarge on hover/reticle focus
                    if (nodeData.type === 'concept') finalTargetColor = new THREE.Color(highlightColor);
                    else if (nodeData.type === 'paper') finalTargetColor = new THREE.Color(paperHighlightColor);
                     // Team nodes are handled separately above, but could be included here too if needed
                }

                // Apply color and scale changes smoothly
                node.material.color.lerp(finalTargetColor, colorLerpFactor);
                node.scale.lerp(new THREE.Vector3(targetScale, targetScale, targetScale), 0.1);
            });

            renderer.render(scene, camera);
        }


        // --- Initialize ---
        window.onload = init;

    </script>
</body>
</html>
